<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-flash.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://quan-sun.github.io").hostname,root:"/",scheme:"Pisces",version:"7.5.0",exturl:!1,sidebar:{position:"right",display:"post",offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},sidebarPadding:40}</script><meta property="og:type" content="website"><meta property="og:title" content="孙泉的博客"><meta property="og:url" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;index.html"><meta property="og:site_name" content="孙泉的博客"><meta property="og:locale" content="zh-CN"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://quan-sun.github.io/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!0,isPost:!1,isPage:!1,isArchive:!1}</script><title>孙泉的博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="孙泉的博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><canvas class="fireworks" style="position:fixed;left:0;top:0;z-index:1;pointer-events:none"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/src/fireworks.js"></script><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">孙泉的博客</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Quan's Blog</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">5</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">3</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">9</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://quan-sun.github.io/2019/12/22/7-problems-of-backtracking-algorithm/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Quan Sun"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="孙泉的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a href="/2019/12/22/7-problems-of-backtracking-algorithm/" class="post-title-link" itemprop="url">7 problems of backtracking algorithm</a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-22 10:23:35 / 修改时间：10:25:40" itemprop="dateCreated datePublished" datetime="2019-12-22T10:23:35-05:00">2019-12-22</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Leetcode/" itemprop="url" rel="index"><span itemprop="name">Leetcode</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Disqus：</span><a title="disqus" href="/2019/12/22/7-problems-of-backtracking-algorithm/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/22/7-problems-of-backtracking-algorithm/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p> A summary for 7 “backtracking” problems in Leetcode. This kind of problems can be solved in a template. It’s easy to find the pattern as following.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// nums: a vector of numbers that you have to traverse</span></span><br><span class="line"><span class="comment">// ans: a vector to record final answer</span></span><br><span class="line"><span class="comment">// path: a vector to record traversing numbers</span></span><br><span class="line"><span class="comment">// start: a start point taht you do DFS in 'nums'</span></span><br><span class="line"><span class="comment">// target: a target of final answer, i.e.the numbers sums to target</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; ans, <span class="keyword">int</span> start, <span class="keyword">int</span> target, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; path)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (target==<span class="number">0</span>) &#123; <span class="comment">// the condition of push path into ans</span></span><br><span class="line">            ans.push_back(path);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = start; i &lt; nums.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (target &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// 'backtarcking - kits': push_back; DFS; pop_back.</span></span><br><span class="line">                <span class="comment">// you will find the next 3 lines of code in all 'backtracking' problems</span></span><br><span class="line">                path.push_back(nums[i]);</span><br><span class="line">                dfs(nums, ans, i, target-nums[i], path);</span><br><span class="line">                path.pop_back();                </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><div class="post-button"> <a class="btn" href="/2019/12/22/7-problems-of-backtracking-algorithm/#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://quan-sun.github.io/2019/12/17/ResNet/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Quan Sun"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="孙泉的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a href="/2019/12/17/ResNet/" class="post-title-link" itemprop="url">ResNet</a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-17 21:55:25 / 修改时间：22:01:42" itemprop="dateCreated datePublished" datetime="2019-12-17T21:55:25-05:00">2019-12-17</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Disqus：</span><a title="disqus" href="/2019/12/17/ResNet/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/17/ResNet/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>paper地址: <a href="https://arxiv.org/pdf/1512.03385v1.pdf" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a></p><p>2015年的ILSVRC冠军是微软的ResNet，152层的ResNet在深度和错误率上都创造了记录。可以说ResNet的出现使得Deep Learning真正的进入了”Deep”的时代，其中所提出的残差学习对之后深度学习的发展产生了深远的影响。</p><p>ResNet到底解决的是什么问题？之前看到有些文章说它解决了梯度消失的问题，但是，这很明显是错误的。ResNet的paper中，作者直接指出了梯度消失的问题在BN提出之后基本上已经得到了解决，而ResNet解决的是degradation的问题。所谓degradation就是，随着网络层数的增加，网络的正确率会饱和，然后迅速退化。</p><div class="post-button"> <a class="btn" href="/2019/12/17/ResNet/#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://quan-sun.github.io/2019/12/16/Inception-Network/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Quan Sun"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="孙泉的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a href="/2019/12/16/Inception-Network/" class="post-title-link" itemprop="url">Inception Network</a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-16 22:24:33 / 修改时间：22:34:05" itemprop="dateCreated datePublished" datetime="2019-12-16T22:24:33-05:00">2019-12-16</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Disqus：</span><a title="disqus" href="/2019/12/16/Inception-Network/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/16/Inception-Network/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>Inception networks也是深度学习的重要的奠基石。相比与之前说的AlexNet和VGG，它具有更深更宽更复杂的结构和更优异的表现，但是它的参数量甚至比AlexNet还要少好几倍。目前InceptionNet已经有了四个版本，分别是Inception v1、Inception v2、Inception v3和Inception v4,每一个版本都是对上一个版本的提升。</p><h2 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception v1"></a>Inception v1</h2><p>paper地址: <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43022.pdf" target="_blank" rel="noopener">Going Deeper with Convolutions</a></p><p>Inception v1也被称为GoogleNet，它的网络结构是基于inception module构建的。</p><p>Inception module的结构如下图所示，</p><div class="post-button"> <a class="btn" href="/2019/12/16/Inception-Network/#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://quan-sun.github.io/2019/12/14/VGG/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Quan Sun"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="孙泉的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a href="/2019/12/14/VGG/" class="post-title-link" itemprop="url">VGG</a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-14 20:39:08 / 修改时间：20:41:23" itemprop="dateCreated datePublished" datetime="2019-12-14T20:39:08-05:00">2019-12-14</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Disqus：</span><a title="disqus" href="/2019/12/14/VGG/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/14/VGG/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p><a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></p><p>VGG由牛津大学的Karen Simonyan和Andrew Zisserman提出，在2014年的ILSVRC中top-5 error rate达到了7.3%。VGG可以说是CNN中最重要的模型之一，它强调了深度在CNN模型中的重要作用(我之前的一篇[博客][<a href="https://quan-sun.github.io/2019/11/24/为什么神经网络越深越好/#more]介绍了深度对于神经网络的影响)。">https://quan-sun.github.io/2019/11/24/为什么神经网络越深越好/#more]介绍了深度对于神经网络的影响)。</a></p><p>VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的。VGG在加深网络层数同时为了避免参数过多，所有层都采用$3\times3$的小卷积核，stride设置为1。VGG的输入图像大小为$224 \times 244$，在训练集上对所有图像计算RGB均值，然后将处理后的图像作为传入VGG卷积网络，使用$3\times3$或者$1\times1$的filter，stride设置为1。</p><div class="post-button"> <a class="btn" href="/2019/12/14/VGG/#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://quan-sun.github.io/2019/12/14/AlexNet/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Quan Sun"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="孙泉的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a href="/2019/12/14/AlexNet/" class="post-title-link" itemprop="url">AlexNet</a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-14 16:20:48 / 修改时间：16:43:33" itemprop="dateCreated datePublished" datetime="2019-12-14T16:20:48-05:00">2019-12-14</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Disqus：</span><a title="disqus" href="/2019/12/14/AlexNet/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/14/AlexNet/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></p><p>AlexNet是2012年ILSVRC的冠军模型，top-5 error rate达到15.3%，甩了第二名10.8%，完全碾压其他传统的依赖hand-craft特征的计算机视觉方法。它的出现绝对是深度学习的里程碑，这是第一次在大规模的数据集上实现了深层卷积网络的结构，从此开始了深度学习的热潮。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/2019/12/14/AlexNet/image-20191214153724495.png" alt="image-20191214153724495"></p><p>AlexNet总共有8哥权重层，其中包括5个卷积层和3个全连接层。</p><p><img src="/2019/12/14/AlexNet/alexnet.png" alt="architecture"></p><p>第1、2卷积层后连有LRN(Local Response Normalization)层，每个LRN及最后层卷积层后跟有最大池化层，并且每个权重层均有RELU激活函数。全连接后使用dropout解决过拟合。</p><div class="post-button"> <a class="btn" href="/2019/12/14/AlexNet/#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://quan-sun.github.io/2019/12/14/Tips-to-make-GANs-work/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Quan Sun"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="孙泉的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a href="/2019/12/14/Tips-to-make-GANs-work/" class="post-title-link" itemprop="url">Tips to make GANs work</a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-14 11:02:41 / 修改时间：11:14:01" itemprop="dateCreated datePublished" datetime="2019-12-14T11:02:41-05:00">2019-12-14</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Disqus：</span><a title="disqus" href="/2019/12/14/Tips-to-make-GANs-work/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/14/Tips-to-make-GANs-work/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>A great <a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">git repo</a> with 7.6k stars about training GANs.</p><p>While research in Generative Adversarial Networks (GANs) continues to improve the fundamental stability of these models, we use a bunch of tricks to train them and make them stable day to day.</p><h3 id="Normalize-the-inputs"><a href="#Normalize-the-inputs" class="headerlink" title="Normalize the inputs"></a>Normalize the inputs</h3><ul><li>normalize the images between -1 and 1</li><li>Tanh as the last layer of the generator output</li></ul><div class="post-button"> <a class="btn" href="/2019/12/14/Tips-to-make-GANs-work/#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://quan-sun.github.io/2019/12/11/Study-Notes-of-Machine-Learning-V1/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Quan Sun"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="孙泉的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a href="/2019/12/11/Study-Notes-of-Machine-Learning-V1/" class="post-title-link" itemprop="url">Study Notes of Machine Learning V1</a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-11 21:12:56 / 修改时间：22:03:58" itemprop="dateCreated datePublished" datetime="2019-12-11T21:12:56-05:00">2019-12-11</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Disqus：</span><a title="disqus" href="/2019/12/11/Study-Notes-of-Machine-Learning-V1/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/11/Study-Notes-of-Machine-Learning-V1/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h3><h4 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a>Bias and Variance</h4><p><code>bias</code> is the difference between ground truth and expectation of prediction of your model, which measures fitting ability of models; <code>variance</code> is the difference between expectation of prediction of your model and prediction, that is difference between results of different datasets feeding in your model, which measures stability of models.</p><div class="post-button"> <a class="btn" href="/2019/12/11/Study-Notes-of-Machine-Learning-V1/#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://quan-sun.github.io/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Quan Sun"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="孙泉的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a href="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/" class="post-title-link" itemprop="url">On the Number of Linear Regions of Deep Neural Networks</a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-09 20:31:50" itemprop="dateCreated datePublished" datetime="2019-12-09T20:31:50-05:00">2019-12-09</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-12-12 19:34:05" itemprop="dateModified" datetime="2019-12-12T19:34:05-05:00">2019-12-12</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Disqus：</span><a title="disqus" href="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h3><p>使用ReLU等非线性单元的神经网络相当于一个分片线性函数，线性区域越多，神经网络的非线性就越强，网络的效果可能就更好。这篇文章讨论了在神经元总数一样的情况下，增加网络的深度可以产生更多的线性区域。文章中使用折纸类比了这个过程。</p><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209181524633.png" title="李宏毅Deep Structure PPT"><p>以ReLU为例，我们知道ReLU单元$y=ReLU(Wx+b)$会在输入空间中产生一个超平面$Wx+b=0$,把输入空间分成两部分，一部分的空间输出值为常数0，另一个空间的输出为$y=Wx+b$。可以看到一个分片线性函数会产生两个线性区域。那么如果输入空间是二维，就像一张纸沿着$Wx+b=0$折一下，再打开，会产生两个线性区域。</p><p>对于一个具有两个ReLU单元的单隐藏层神经网络，$y_1=ReLU(W_1x+b_1)$和$y_2=ReLU(W_2x+b_2)$，输出为$y=y_1+y_2$。那么这个神经网络就有四个线性区域，每个区域的输出值为0、$W_1x+b_1$、$W_2x+b_2$和$(W_1+W_2)x+(b_1+b_2)$。这就像一张纸，先沿着$W_1x+b_1=0$折一下，打开，再沿着$W_2x+b_2=0$折一下，会产生四个区域。所以如果只有一个隐藏层，单纯地增加神经元的个数，相当于重复上述操作，不断的沿着一条线折一下，再打开。对于一张二维的纸而言，折叠N此所产生的线性区域不超过$\frac{N^2+N}{2}+1$.</p><div class="post-button"> <a class="btn" href="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://quan-sun.github.io/2019/11/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Quan Sun"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="孙泉的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> <a href="/2019/11/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD/" class="post-title-link" itemprop="url">为什么神经网络越深越好</a></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-11-24 22:32:55" itemprop="dateCreated datePublished" datetime="2019-11-24T22:32:55-05:00">2019-11-24</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-11-25 00:13:49" itemprop="dateModified" datetime="2019-11-25T00:13:49-05:00">2019-11-25</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Disqus：</span><a title="disqus" href="/2019/11/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/11/24/为什么神经网络越深越好/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p>最近在StackExchange上看到的一个帖子，<a href="https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w" target="_blank" rel="noopener">What is the difference between a neural network and a deep neural network, and why do the deep ones work better?</a>，讨论的是“深度”对于神经网络的影响。从下面的图片可以看到ImageNet中的冠军模型也越来越深。</p><img src="/2019/11/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD/20191124-1.png" width="80%" height="80%" title="ImageNet" alt="ImageNet"><p>在Deep Learning那本书中，我们也能看到一样的结果。</p><img src="/2019/11/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD/20191124-2.png" width="80%" height="80%" title="Book: Deep Learning" alt="Book: Deep Learning"><p>为什么网络越深模型的表现越好呢？下面是不同角度对这个问题的回答。</p><p>在知乎的帖子<a href="https://www.zhihu.com/question/313633835" target="_blank" rel="noopener">网络深度对深度学习模型性能有什么影响？</a>中，答主言有三从两个方面解释了<u>为什么加深可以提升性能</u>：<strong>1.更好的拟合特征，更强大的表达能力。更深的模型，意味着更好的非线性表达能力，可以学习更加复杂的变换，从而可以拟合更加复杂的特征输入。2.网络更深，每一层要做的事情也更加简单了，可以更好地进行逐层学习。</strong></p><div class="post-button"> <a class="btn" href="/2019/11/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD/#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">Quan Sun</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">3</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">5</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/Quan-Sun" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Quan-Sun" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> GitHub</a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2019</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Quan Sun</span></div><div class="busuanzi-count"><script pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="199" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js"></script><script size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/pjax/pjax.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.getAttribute('pjax') !== null) {
      element.setAttribute('pjax', '');
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script src="/js/local-search.js"></script><div id="pjax"><script type="text/x-mathjax-config">
    MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><script>function loadCount(){var d=document,n=d.createElement("script");n.src="https://quan.disqus.com/count.js",n.id="dsq-count-scr",(d.head||d.body).appendChild(n)}window.addEventListener("load",loadCount,!1)</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",model:{jsonPath:"live2d-widget-model-wanko"},display:{position:"right",width:150,height:300},mobile:{show:!0},log:!1,tagMode:!1})</script></body></html>