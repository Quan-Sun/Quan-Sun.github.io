<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-flash.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://quan-sun.github.io").hostname,root:"/",scheme:"Pisces",version:"7.5.0",exturl:!1,sidebar:{position:"right",display:"post",offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},sidebarPadding:40}</script><meta name="description" content="TL;DR使用ReLU等非线性单元的神经网络相当于一个分片线性函数，线性区域越多，神经网络的非线性就越强，网络的效果可能就更好。这篇文章讨论了在神经元总数一样的情况下，增加网络的深度可以产生更多的线性区域。文章中使用折纸类比了这个过程。以ReLU为例，我们知道ReLU单元$y=ReLU(Wx+b)$会在输入空间中产生一个超平面$Wx+b=0$,把输入空间分成两部分，一部分的空间输出值为常数0，另一"><meta name="keywords" content="学习笔记,深度学习"><meta property="og:type" content="article"><meta property="og:title" content="On the Number of Linear Regions of Deep Neural Networks"><meta property="og:url" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;index.html"><meta property="og:site_name" content="孙泉的博客"><meta property="og:description" content="TL;DR使用ReLU等非线性单元的神经网络相当于一个分片线性函数，线性区域越多，神经网络的非线性就越强，网络的效果可能就更好。这篇文章讨论了在神经元总数一样的情况下，增加网络的深度可以产生更多的线性区域。文章中使用折纸类比了这个过程。以ReLU为例，我们知道ReLU单元$y=ReLU(Wx+b)$会在输入空间中产生一个超平面$Wx+b=0$,把输入空间分成两部分，一部分的空间输出值为常数0，另一"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209181524633.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191208183425173.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209001503997.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209003254040.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209003328371.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209003729020.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209130708933.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209133906134.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209140301680.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209153956151.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209154657346.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209155554775.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209161748533.png"><meta property="og:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209162932406.png"><meta property="og:updated_time" content="2019-12-13T00:34:05.287Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;quan-sun.github.io&#x2F;2019&#x2F;12&#x2F;09&#x2F;On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks&#x2F;image-20191209181524633.png"><link rel="canonical" href="https://quan-sun.github.io/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>On the Number of Linear Regions of Deep Neural Networks | 孙泉的博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="孙泉的博客" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><canvas class="fireworks" style="position:fixed;left:0;top:0;z-index:1;pointer-events:none"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/src/fireworks.js"></script><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">孙泉的博客</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Quan's Blog</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">3</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">2</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">4</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://quan-sun.github.io/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="Quan Sun"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="孙泉的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> On the Number of Linear Regions of Deep Neural Networks</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-09 20:31:50" itemprop="dateCreated datePublished" datetime="2019-12-09T20:31:50-05:00">2019-12-09</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-12-12 19:34:05" itemprop="dateModified" datetime="2019-12-12T19:34:05-05:00">2019-12-12</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Disqus：</span><a title="disqus" href="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h3><p>使用ReLU等非线性单元的神经网络相当于一个分片线性函数，线性区域越多，神经网络的非线性就越强，网络的效果可能就更好。这篇文章讨论了在神经元总数一样的情况下，增加网络的深度可以产生更多的线性区域。文章中使用折纸类比了这个过程。</p><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209181524633.png" title="李宏毅Deep Structure PPT"><p>以ReLU为例，我们知道ReLU单元$y=ReLU(Wx+b)$会在输入空间中产生一个超平面$Wx+b=0$,把输入空间分成两部分，一部分的空间输出值为常数0，另一个空间的输出为$y=Wx+b$。可以看到一个分片线性函数会产生两个线性区域。那么如果输入空间是二维，就像一张纸沿着$Wx+b=0$折一下，再打开，会产生两个线性区域。</p><p>对于一个具有两个ReLU单元的单隐藏层神经网络，$y_1=ReLU(W_1x+b_1)$和$y_2=ReLU(W_2x+b_2)$，输出为$y=y_1+y_2$。那么这个神经网络就有四个线性区域，每个区域的输出值为0、$W_1x+b_1$、$W_2x+b_2$和$(W_1+W_2)x+(b_1+b_2)$。这就像一张纸，先沿着$W_1x+b_1=0$折一下，打开，再沿着$W_2x+b_2=0$折一下，会产生四个区域。所以如果只有一个隐藏层，单纯地增加神经元的个数，相当于重复上述操作，不断的沿着一条线折一下，再打开。对于一张二维的纸而言，折叠N此所产生的线性区域不超过$\frac{N^2+N}{2}+1$.</p><a id="more"></a><p>对于两个隐藏层的神经网络。第二层的一个ReLU单元会在第二层的输入空间（第一层的输出值域）上形成一个超平面，并将第一层的值域分成两个区域。而我们注意到，第一层可能会把不同的输入映射到相同的输出值，因此在第一层的值域上又多一个区域，就可能再输入空间上多出好几个区域。这就像拿一张纸，折几下，折出几个区域，然后不打开，再折一下。那么最后折的这一下，就会在之前折出的多个区域中新增一个区域。这就是增加层数带来的效果。对于更深层的网络，就像拿一张纸，一直折N次，折到最后才打开，那么这些折痕最多可以把纸分割成$2^N$个区域。</p><h3 id="1-Introduction-amp-Conclusion"><a href="#1-Introduction-amp-Conclusion" class="headerlink" title="1. Introduction &amp; Conclusion"></a>1. Introduction &amp; Conclusion</h3><p>这篇文章研究了深度前馈神经网络可根据它们的线性区域数计算函数的复杂性；讨论了深层模型的每一层都能够识别其输入片段，使得层与层的组合能够识别指数数量的输入区域。这导致指数地复制在更高层中计算的函数的复杂性。深度模型以这种方式计算的函数是复杂的，但是它们仍然具有由复制引起的intrinsic rigidity，这可能有助于深度模型比浅模型更好地泛化到看不见的样本。线性区域的结构取决于单元的类型，例如hyperplane arrangements for shallow rectifier和Voronoi diagrams for shallow maxout networks。在maxout和rectifier网络中，随着深度增加区域数量都指数地增长。给定网络的参数空间被划分为目标函数具有相应线性区域的区域。</p><p>在许多隐藏层的渐近极限中，尽管使用相同数量的计算单元，深层网络能够将它们的输入空间分成比浅层网络指数地更线性的响应区域。这篇文章描述了那些模型的中间层如何将它们的几个输入映射到同一个输出。以这种方式计算的函数的分层组合经常随着层数的增加而指数地重复使用低级计算。这一关键属性使深层网络能够计算高度复杂和结构化的函数。通过估计可由两种重要类型的分段线性网络（rectifier units和maxout units）计算的函数的线性区域的数量来支持这个想法。</p><p>给定模型可以计算的函数的线性区域的数量是模型灵活性的度量。如图1所示，比较了相同隐藏神经元数量的单层和双层网络学到的决策边界，体现了深度的优势；深层模型更精确地捕捉到所需的边界，用大量的线性pieces来逼近它。深层网络能够通过将输入邻域映射到中间隐藏层的一个公共输出来识别指数数量的输入邻域。对该中间层的激活的计算被复制多次，在每个被识别的邻域中复制一次。这允许网络计算非常复杂的函数，即使它们是用相对较少的参数定义的。参数的数量是可由网络计算的函数的维度的上限，而少量参数意味着可计算函数具有低维度。深度前馈分段线性网络可计算的一组函数即使维度很低，但通过在层与层之间重复使用和组合特征，实现了指数复杂性。</p> <img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191208183425173.png"><h3 id="2-Feedforward-Neural-Networks-and-their-Compositional-Properties"><a href="#2-Feedforward-Neural-Networks-and-their-Compositional-Properties" class="headerlink" title="2.Feedforward Neural Networks and their Compositional Properties"></a>2.Feedforward Neural Networks and their Compositional Properties</h3><p>本节讨论了深度前馈网络通过使用相对较少的计算单元来重新映射其输入空间以创建复杂对称性的能力。分析了深度模型的每一层都能够将其输入的不同区域映射到一个公共输出。这导致了一种复合结构，其中在给定层上产生相同输出的所有输入区域中，有效地复制了较高层上的计算。在输入空间上复制计算的能力随网络层的数量呈指数增长。</p><h4 id="2-1-Shallow-Neural-Networks"><a href="#2-1-Shallow-Neural-Networks" class="headerlink" title="2.1 Shallow Neural Networks"></a>2.1 Shallow Neural Networks</h4> 根据输入的不同，Rectifier units有两种行为，等于0或者线性。这两个行为之间的边界由超平面给出，来自rectifier层中所有单元的所有超平面的集合形成一个hyperplane arrangement。arrangement中的超平面将输入空间分为几个区域。arrangement的区域数量可以根据arrangement的特征函数给出。在$R^{n_0}$中一个$n_1$的arrangement有最多$\sum_{j=0}^{n_0} {{n_1} \choose j}$个区域。当且仅当超平面处于一般位置时，才能实现这个数量的区域。这意味着由shallow rectifier network（$n_0$的输入和$n_1$的隐藏单元）计算的函数的线性区域的最大数量是$\sum_{j=0}^{n_0} {n_1 \choose j}$。<h4 id="2-2-Deep-Neural-Networks"><a href="#2-2-Deep-Neural-Networks" class="headerlink" title="2.2 Deep Neural Networks"></a>2.2 Deep Neural Networks</h4><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209001503997.png"><p>由前馈网络的L层对L-1层的一组激活进行的计算可有效地对导致L-1层相同激活的所有输入空间区域进行。可以选择给定层的输入权重和偏差，使得计算的函数在输入空间中具有最大数量的preimages的前一层的那些激活值上表现出最有趣的行为，从而在输入空间中多次重复有趣的计算和生成complicated-looking的函数。</p> <img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209003254040.png"> <img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209003328371.png"><p>构造具有许多线性区域的函数的想法是使用前L-1个隐藏层来标识许多输入空间邻域，并将它们全部映射到L-1隐层的激活邻域$P^L$，每个激活邻域$P^L$都属于一个不同的最后一个隐藏层的线性区域。</p><h4 id="2-34-Identification-of-Inputs-as-Space-Foldings"><a href="#2-34-Identification-of-Inputs-as-Space-Foldings" class="headerlink" title="2.34 Identification of Inputs as Space Foldings"></a>2.34 Identification of Inputs as Space Foldings</h4><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209003729020.png"><p>识别两个子集$S$和$S’$的映射$F$可以看成是折叠它domain的一种算子，折叠后$S$和$S’$被映射到相同的输出。如图2所示，absolute value function折叠它自己的domain两次（两个坐标轴各一次）。这种折叠识别了二维欧几里德空间的四个象限。通过组成这样的操作，相同类型的映射可以再次应用于输出，以便重新折叠之前的第一次折叠。深层神经网络的每个隐藏层都和一种折叠算子相关。每个隐藏层折叠前一层的激活空间。反过来，深层神经网络从第一层开始有效地递归折叠其输入空间。这种递归折叠的结果是，在最终折叠空间上计算的任何函数都将应用于由对应于连续折叠的映射所识别的所有的折叠子集。这意味着在深度模型中，最后一层的图像空间的任何划分都被复制到由一连串折叠识别的所有输入空间区域中。</p><p>空间折叠不限于沿坐标轴折叠，也不必保留长度。相反，空间被折叠是根据输入权重$W$和偏置$b$中编码的方向和偏移以及在每个隐藏层使用的非线性激活函数。这意味着所识别的输入空间区域的大小和方向可以彼此不同。在激活函数不是分段线性的情况下，折叠操作可能更加复杂。</p> <img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209130708933.png"><h3 id="3-Deep-Rectifier-Network"><a href="#3-Deep-Rectifier-Network" class="headerlink" title="3.Deep Rectifier Network"></a>3.Deep Rectifier Network</h3><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209133906134.png"><h4 id="3-1-Illustration-of-the-Construction"><a href="#3-1-Illustration-of-the-Construction" class="headerlink" title="3.1 Illustration of the Construction"></a>3.1 Illustration of the Construction</h4><p>考虑一层网络有$n$个rectifiers，$n_0$个输入变量，其中$n \geq n_0$。将rectifier单元集划分为基数为$p=[n / n_0]$的$n_0$个(非重叠)子集，忽略多余单元。考虑第$j$个子集中的单元。</p> <img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209140301680.png"><p>$\tilde h$是由$h$通过与线性函数组合产生的。这个线性函数可以有效地被下一层的预激活函数吸收。因此，我们可以将$\tilde h$视为由当前层计算的函数。作为此rectifier层的单位超立方体输出的函数，由较深层进行的计算将复制到$p^{n_0}$个已识别输入空间超立方体的每一个上。</p><h4 id="3-2-Formal-Result"><a href="#3-2-Formal-Result" class="headerlink" title="3.2 Formal Result"></a>3.2 Formal Result</h4><p>可以将上面所描述的construction泛化为一个深层rectifier网络，有$n_0$的输入和$L$层宽度为$n_i$的隐藏层，对于所有$i \in [L]$，$n_i \geq n_0$。那么这个深层rectifier网络的线性区域的最大数量的下限为：</p> <img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209153956151.png"><p>下一个推论给出这些边界的渐近行为的表达式。假设对于所有$i \geq 1$都有$n_0=\Omega (1)$和$n_i=n_0$，一个有$Ln$个隐藏单元的单层网络的区域数量表现为$\Omega (L^{n_0}n^{n_0})$。但是，对于一个深度网络，</p> <img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209154657346.png"><p>因此，深层模型的线性区域数量呈指数增长(in $L$)，以及呈多项式增长(in $n$)，这比具有$nL$个隐藏单元的浅层模型的线性区域快得多。结果表明，即使$L$和$n$较小，深的rectifier模型也比浅的rectifier模型能够产生更多的线性区域。</p><h3 id="4-Deep-Maxout-Networks"><a href="#4-Deep-Maxout-Networks" class="headerlink" title="4.Deep Maxout Networks"></a>4.Deep Maxout Networks</h3><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209155554775.png"><p>由于两个凸函数的最大值为凸，因此maxout单元和maxout层将计算凸函数。函数集合的最大值被称为<em>upper envelope</em>。一个maxout单元线性区域的最大数量等于它的rank(上式中的k)。</p><p>maxout层的线性区域是各个maxout单元的线性区域的交集。为了获得该层的线性区域的数量，需要描述每个maxout单元的线性区域的结构，并研究它们可能的交集。Voronoi图可以提升到线性函数的upper envelopes，因此它们描述了由maxout单元生成的输入空间分区。</p> <img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209161748533.png"><p>可以通过一个具有两倍数量单元的rectifier层来模拟一个rank-2 maxout层。一个有$L-1$个宽度$n=n_0$的隐藏层的rank-2 maxout网络可以识别出$2^{n_0(L-1)}$个输入空间区域。反过来看，它可以计算有$2^{n_0(L-1)}2^{n_0}=2^{n_0L}$个线性区域的函数。对于rank-k，一个rank-k maxout单元可以从它的输入域识别k个cones，每个cone都与一个对应线性方程$f_i$的梯度$W_i$的positive half-ray ${rW_i \in R^n: r \in R_+ }$相邻。</p> <img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209162932406.png"><p>所以深层maxout网络可以计算具有多个线性区域的函数，这些线性区域随层数成指数增长，并且比具有相同数量单元的浅层模型的最大区域数更快地成指数增长。与rectifier模型相似，该指数行为也可以根据网络参数的数量来确定。尽管可以由maxout层计算的某些函数也可以由rectifier层计算，但是从最后一节开始的rectifier构造会出现maxout网络无法计算的函数。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://www.zhihu.com/question/62705160/answer/201377484" target="_blank" rel="noopener">https://www.zhihu.com/question/62705160/answer/201377484</a></p><p>[2] <a href="https://arxiv.org/pdf/1402.1869.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1402.1869.pdf</a></p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 学习笔记</a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2019/11/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD/" rel="prev" title="为什么神经网络越深越好"><i class="fa fa-chevron-left"></i> 为什么神经网络越深越好</a></div><div class="post-nav-item"> <a href="/2019/12/11/Study-Notes-of-Machine-Learning-V1/" rel="next" title="Study Notes of Machine Learning V1">Study Notes of Machine Learning V1<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#TL-DR"><span class="nav-number">1.</span> <span class="nav-text">TL;DR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Introduction-amp-Conclusion"><span class="nav-number">2.</span> <span class="nav-text">1. Introduction &amp; Conclusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Feedforward-Neural-Networks-and-their-Compositional-Properties"><span class="nav-number">3.</span> <span class="nav-text">2.Feedforward Neural Networks and their Compositional Properties</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Shallow-Neural-Networks"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 Shallow Neural Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Deep-Neural-Networks"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 Deep Neural Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-34-Identification-of-Inputs-as-Space-Foldings"><span class="nav-number">3.3.</span> <span class="nav-text">2.34 Identification of Inputs as Space Foldings</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Deep-Rectifier-Network"><span class="nav-number">4.</span> <span class="nav-text">3.Deep Rectifier Network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Illustration-of-the-Construction"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 Illustration of the Construction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Formal-Result"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Formal Result</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Deep-Maxout-Networks"><span class="nav-number">5.</span> <span class="nav-text">4.Deep Maxout Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reference"><span class="nav-number">6.</span> <span class="nav-text">Reference</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">Quan Sun</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">2</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">3</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/Quan-Sun" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Quan-Sun" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> GitHub</a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2019</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Quan Sun</span></div><div class="busuanzi-count"><script pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="199" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js"></script><script size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/pjax/pjax.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.getAttribute('pjax') !== null) {
      element.setAttribute('pjax', '');
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script><script src="/js/local-search.js"></script><div id="pjax"><script type="text/x-mathjax-config">
    MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><script>function loadCount(){var d=document,n=d.createElement("script");n.src="https://quan.disqus.com/count.js",n.id="dsq-count-scr",(d.head||d.body).appendChild(n)}window.addEventListener("load",loadCount,!1)</script><script>function loadComments(){if(window.DISQUS)DISQUS.reset({reload:!0,config:{page:{url:"https://quan-sun.github.io/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/",identifier:"2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/",title:"On the Number of Linear Regions of Deep Neural Networks"}}});else{var e=document,t=e.createElement("script");t.src="https://quan.disqus.com/embed.js",t.setAttribute("data-timestamp",""+ +new Date),(e.head||e.body).appendChild(t)}}window.addEventListener("load",loadComments,!1)</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",model:{jsonPath:"live2d-widget-model-wanko"},display:{position:"right",width:150,height:300},mobile:{show:!0},log:!1,tagMode:!1})</script></body></html>