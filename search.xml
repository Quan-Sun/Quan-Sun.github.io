<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AlexNet</title>
    <url>/2019/12/14/AlexNet/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></p><p>AlexNet是2012年ILSVRC的冠军模型，top-5 error rate达到15.3%，甩了第二名10.8%，完全碾压其他传统的依赖hand-craft特征的计算机视觉方法。它的出现绝对是深度学习的里程碑，这是第一次在大规模的数据集上实现了深层卷积网络的结构，从此开始了深度学习的热潮。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/2019/12/14/AlexNet/image-20191214153724495.png" alt="image-20191214153724495"></p><p>AlexNet总共有8哥权重层，其中包括5个卷积层和3个全连接层。</p><p><img src="/2019/12/14/AlexNet/alexnet.png" alt="architecture"></p><p>第1、2卷积层后连有LRN(Local Response Normalization)层，每个LRN及最后层卷积层后跟有最大池化层，并且每个权重层均有RELU激活函数。全连接后使用dropout解决过拟合。</p><a id="more"></a>





<table>
<thead>
<tr>
<th align="center">layer</th>
<th align="center">Stride</th>
<th align="center">Kernel size</th>
<th align="center">feature map</th>
<th align="center">PADDING</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Conv1</td>
<td align="center">4</td>
<td align="center">$11\times11\times96$</td>
<td align="center">$55\times55\times96$</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">Pooling1</td>
<td align="center">2</td>
<td align="center">$3\times3$</td>
<td align="center">$27\times27\times96$</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">Conv2</td>
<td align="center">1</td>
<td align="center">$5\times5\times256$</td>
<td align="center">$27\times27\times256$</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">Pooling2</td>
<td align="center">2</td>
<td align="center">$3\times3$</td>
<td align="center">$13\times13\times256$</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">Conv3</td>
<td align="center">1</td>
<td align="center">$3\times3\times384$</td>
<td align="center">$13\times13\times384$</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">Conv4</td>
<td align="center">1</td>
<td align="center">$3\times3\times384$</td>
<td align="center">$13\times13\times384$</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">Conv5</td>
<td align="center">1</td>
<td align="center">$3\times3\times256$</td>
<td align="center">$13\times13\times256$</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">Pooling5</td>
<td align="center">2</td>
<td align="center">$3\times3$</td>
<td align="center">$6\times6\times256$</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">FC6</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">4096</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">FC7</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">4096</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">FC8</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">1000</td>
<td align="center"></td>
</tr>
</tbody></table>
<h2 id="网络特点"><a href="#网络特点" class="headerlink" title="网络特点"></a>网络特点</h2><p>1.Training on Multiple GPUs</p>
<p>2.ReLU Nonlinearity</p>
<p>传统神经网络激活函数通常为反正切或是sigmoid，AlexNet使用RELU作为激活函数，相比于反正切，该方法训练速度大约有6倍提升。</p>
<p>3.Local Response Normalization</p>
<p><img src="/2019/12/14/AlexNet/image-20191214160951499.png" alt="LRN"></p>
<p>LRN加在RELU的激活后面，能够增加网络的泛化能力，并在ILSVRC-2012上降低1%的错误率。不过后来的网络证明了LRN并非CNN中必须包含的层，甚至有些网络加入LRN后模型表现反而降低了。</p>
<p>4.Overlapping Pooling</p>
<p>重叠池化是指池化步伐小于kernel size，池化后的feature map的感受野其实是有部分重叠的,文章指出这样做可以防止过拟合。但是，后面的网络鲜有继续使用的，因为有更好的防止过拟合的技术出现，比如BN已经可以解决过拟合的问题。</p>
<p>5.Data Augmentation</p>
<p>使用Random Crop和flip进行数据增强，从而扩充训练样本的数量。具体做法就是将图片resize到$256\times256$，然后从中随机crop出$224\times224$大小的patch训练；测试时从四个角和中心分别crop，然后水平翻转图像，这就得到了10张测试图片，然后对于这10张测试图片得到的结果取平均值。另一种数据增强方法是转换RGB通道的强度，大约降低了1% error rate，但是这个方法之后的模型很少使用了。</p>
<p>6.Dropout</p>
<p>Dropout通过训练时随机丢弃部分神经元从而减少过拟合的风险。这么做也可以迫使这些神经元去学习互补的一些特征。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>深度学习</tag>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>Tips to make GANs work</title>
    <url>/2019/12/14/Tips-to-make-GANs-work/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>A great <a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">git repo</a> with 7.6k stars about training GANs.</p><p>While research in Generative Adversarial Networks (GANs) continues to improve the fundamental stability of these models, we use a bunch of tricks to train them and make them stable day to day.</p><h3 id="Normalize-the-inputs"><a href="#Normalize-the-inputs" class="headerlink" title="Normalize the inputs"></a>Normalize the inputs</h3><ul>
<li>normalize the images between -1 and 1</li>
<li>Tanh as the last layer of the generator output</li>
</ul><a id="more"></a>


<h3 id="A-modified-loss-function"><a href="#A-modified-loss-function" class="headerlink" title="A modified loss function"></a>A modified loss function</h3><p>In GAN papers, the loss function to optimize G is <code>min (log 1-D)</code>, but in practice folks practically use <code>max log D</code></p>
<ul>
<li>because the first formulation has vanishing gradients early on Goodfellow et. al (2014)</li>
</ul>
<p>In practice, works well:</p>
<ul>
<li>Flip labels when training generator: real = fake, fake = real</li>
</ul>
<p>Use a spherical Z</p>
<ul>
<li>Don’t sample from a Uniform distribution</li>
</ul>
<p><img src="/2019/12/14/Tips-to-make-GANs-work/cube.png" alt="cube" title="Cube"></p>
<ul>
<li>Sample from a gaussian distribution</li>
</ul>
<p><img src="/2019/12/14/Tips-to-make-GANs-work/sphere.png" alt="sphere" title="Sphere"></p>
<ul>
<li>When doing interpolations, do the interpolation via a great circle, rather than a straight line from point A to point B</li>
<li>Tom White’s <a href="https://arxiv.org/abs/1609.04468" target="_blank" rel="noopener">Sampling Generative Networks</a> ref code <a href="https://github.com/dribnet/plat" target="_blank" rel="noopener">https://github.com/dribnet/plat</a> has more details</li>
</ul>
<h3 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a>BatchNorm</h3><ul>
<li>Construct different mini-batches for real and fake, i.e. each mini-batch needs to contain only all real images or all generated images.</li>
<li>when batchnorm is not an option use instance normalization (for each sample, subtract mean and divide by standard deviation).</li>
</ul>
<p><img src="/2019/12/14/Tips-to-make-GANs-work/batchmix.png" alt="batchmix" title="BatchMix"></p>
<h3 id="Avoid-Sparse-Gradients-ReLU-MaxPool"><a href="#Avoid-Sparse-Gradients-ReLU-MaxPool" class="headerlink" title="Avoid Sparse Gradients: ReLU, MaxPool"></a>Avoid Sparse Gradients: ReLU, MaxPool</h3><ul>
<li>the stability of the GAN game suffers if you have sparse gradients</li>
<li>LeakyReLU = good (in both G and D)</li>
<li>For Downsampling, use: Average Pooling, Conv2d + stride</li>
<li>For Up sampling, use: Pixel Shuffle, ConvTranspose2d + stride<ul>
<li>Pixel Shuffle: <a href="https://arxiv.org/abs/1609.05158" target="_blank" rel="noopener">https://arxiv.org/abs/1609.05158</a></li>
</ul>
</li>
</ul>
<h3 id="Use-Soft-and-Noisy-Labels"><a href="#Use-Soft-and-Noisy-Labels" class="headerlink" title="Use Soft and Noisy Labels"></a>Use Soft and Noisy Labels</h3><ul>
<li>Label Smoothing, i.e. if you have two target labels: Real=1 and Fake=0, then for each incoming sample, if it is real, then replace the label with a random number between 0.7 and 1.2, and if it is a fake sample, replace it with 0.0 and 0.3 (for example).<ul>
<li>Salimans et. al. 2016</li>
</ul>
</li>
<li>make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator</li>
</ul>
<h3 id="DCGAN-Hybrid-Models"><a href="#DCGAN-Hybrid-Models" class="headerlink" title="DCGAN / Hybrid Models"></a>DCGAN / Hybrid Models</h3><ul>
<li>Use DCGAN when you can. It works!</li>
<li>if you cant use DCGANs and no model is stable, use a hybrid model :  KL + GAN or VAE + GAN</li>
</ul>
<h3 id="Use-stability-tricks-from-RL"><a href="#Use-stability-tricks-from-RL" class="headerlink" title="Use stability tricks from RL"></a>Use stability tricks from RL</h3><ul>
<li>Experience Replay<ul>
<li>Keep a replay buffer of past generations and occasionally show them</li>
<li>Keep checkpoints from the past of G and D and occasionally swap them out for a few iterations</li>
</ul>
</li>
<li>All stability tricks that work for deep deterministic policy gradients</li>
<li>See Pfau &amp; Vinyals (2016)</li>
</ul>
<h3 id="Use-the-ADAM-Optimizer"><a href="#Use-the-ADAM-Optimizer" class="headerlink" title="Use the ADAM Optimizer"></a>Use the ADAM Optimizer</h3><ul>
<li>optim.Adam rules!<ul>
<li>See Radford et. al. 2015</li>
</ul>
</li>
<li>Use SGD for discriminator and ADAM for generator</li>
</ul>
<h3 id="Track-failures-early"><a href="#Track-failures-early" class="headerlink" title="Track failures early"></a>Track failures early</h3><ul>
<li>D loss goes to 0: failure mode</li>
<li>check norms of gradients: if they are over 100 things are screwing up</li>
<li>when things are working, D loss has low variance and goes down over time vs having huge variance and spiking</li>
<li>if loss of generator steadily decreases, then it’s fooling D with garbage (says martin)</li>
</ul>
<h3 id="Don’t-balance-loss-via-statistics-unless-you-have-a-good-reason-to"><a href="#Don’t-balance-loss-via-statistics-unless-you-have-a-good-reason-to" class="headerlink" title="Don’t balance loss via statistics (unless you have a good reason to)"></a>Don’t balance loss via statistics (unless you have a good reason to)</h3><ul>
<li>Don’t try to find a (number of G / number of D) schedule to uncollapse training</li>
<li>It’s hard and we’ve all tried it.</li>
<li>If you do try it, have a principled approach to it, rather than intuition</li>
</ul>
<p>For example</p>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> lossD &gt; <span class="string">A:</span></span><br><span class="line">  train D</span><br><span class="line"><span class="keyword">while</span> lossG &gt; <span class="string">B:</span></span><br><span class="line">  train G</span><br></pre></td></tr></table></figure>

<h3 id="If-you-have-labels-use-them"><a href="#If-you-have-labels-use-them" class="headerlink" title="If you have labels, use them"></a>If you have labels, use them</h3><ul>
<li>if you have labels available, training the discriminator to also classify the samples: auxiliary GANs</li>
</ul>
<h3 id="Add-noise-to-inputs-decay-over-time"><a href="#Add-noise-to-inputs-decay-over-time" class="headerlink" title="Add noise to inputs, decay over time"></a>Add noise to inputs, decay over time</h3><ul>
<li>Add some artificial noise to inputs to D (Arjovsky et. al., Huszar, 2016)<ul>
<li><a href="http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/" target="_blank" rel="noopener">http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/</a></li>
<li><a href="https://openreview.net/forum?id=Hk4_qw5xe" target="_blank" rel="noopener">https://openreview.net/forum?id=Hk4_qw5xe</a></li>
</ul>
</li>
<li>adding gaussian noise to every layer of generator (Zhao et. al. EBGAN)<ul>
<li>Improved GANs: OpenAI code also has it (commented out)</li>
</ul>
</li>
</ul>
<h3 id="not-sure-Train-discriminator-more-sometimes"><a href="#not-sure-Train-discriminator-more-sometimes" class="headerlink" title="[not-sure] Train discriminator more (sometimes)"></a>[not-sure] Train discriminator more (sometimes)</h3><ul>
<li>especially when you have noise</li>
<li>hard to find a schedule of number of D iterations vs G iterations</li>
</ul>
<h3 id="not-sure-Batch-Discrimination"><a href="#not-sure-Batch-Discrimination" class="headerlink" title="[not-sure] Batch Discrimination"></a>[not-sure] Batch Discrimination</h3><ul>
<li>Mixed results</li>
</ul>
<h3 id="Discrete-variables-in-Conditional-GANs"><a href="#Discrete-variables-in-Conditional-GANs" class="headerlink" title="Discrete variables in Conditional GANs"></a>Discrete variables in Conditional GANs</h3><ul>
<li>Use an Embedding layer</li>
<li>Add as additional channels to images</li>
<li>Keep embedding dimensionality low and upsample to match image channel size</li>
</ul>
<h3 id="Use-Dropouts-in-G-in-both-train-and-test-phase"><a href="#Use-Dropouts-in-G-in-both-train-and-test-phase" class="headerlink" title="Use Dropouts in G in both train and test phase"></a>Use Dropouts in G in both train and test phase</h3><ul>
<li>Provide noise in the form of dropout (50%).</li>
<li>Apply on several layers of our generator at both training and test time</li>
<li><a href="https://arxiv.org/pdf/1611.07004v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.07004v1.pdf</a></li>
</ul>
<h3 id="not-sure-Batch-Discrimination-1"><a href="#not-sure-Batch-Discrimination-1" class="headerlink" title="[not-sure] Batch Discrimination"></a>[not-sure] Batch Discrimination</h3><ul>
<li>Train D and G standalone without updating weights of the other and make sure the losses go down.</li>
</ul>
<h3 id="Authors"><a href="#Authors" class="headerlink" title="Authors"></a>Authors</h3><ul>
<li>Soumith Chintala</li>
<li>Emily Denton</li>
<li>Martin Arjovsky</li>
<li>Michael Mathieu</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Study Notes of Machine Learning V1</title>
    <url>/2019/12/11/Study-Notes-of-Machine-Learning-V1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h3><h4 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a>Bias and Variance</h4><p><code>bias</code> is the difference between ground truth and expectation of prediction of your model, which measures fitting ability of models; <code>variance</code> is the difference between expectation of prediction of your model and prediction, that is difference between results of different datasets feeding in your model, which measures stability of models.</p><a id="more"></a>
<p>In supervised learning, model’s generalized error consists of bias, variance and noise, which can be formulated by $Err(x)={Bias}^2+Variance+IrreducibleError$.</p>
<p><code>bias</code> and <code>variance</code> are standing in two sides of a balance, so reduction of one will increase the other. Obviously, what we do in machine learning is bias-variance trade-off. Taking neural networks as an example, NNs have strong fitting ability, which means they have low bias and high variance, so we need to reduce variance with increasing bias.  </p>
<h4 id="Overfitting-and-Underfitting"><a href="#Overfitting-and-Underfitting" class="headerlink" title="Overfitting and Underfitting"></a>Overfitting and Underfitting</h4><p><code>overfitting</code> and <code>underfitting</code> are associated with <code>bias</code> and <code>variance</code>. Low bias and high variance lead to overfitting, which means your model is too complex for real data and of low generalization; Low variance and high bias lead to underfitting, which means your model is too simple to fit real data.</p>
<p>For a specific task, with increasing of model capacity (it represents complexity of models, which can be understood of number of parameters) the bias will reduce and variance will increase. Obviously, there is an optimal capacity leading best bias-variance trade-off.</p>
<p>Solutions for underfitting:</p>
<ul>
<li>More features</li>
<li>More complex model</li>
</ul>
<p>Solutions for overfitting:</p>
<ul>
<li><p>Data augmentation</p>
</li>
<li><p>Early stopping</p>
</li>
<li><p>Regularization</p>
</li>
<li><p>Dropout </p>
</li>
<li><p>Ensemble </p>
</li>
<li><p><del>Batch Normalization</del>(BN can’t prevent overfitting according to <a href="https://arxiv.org/pdf/1611.03530.pdf" target="_blank" rel="noopener">paper</a>, it just can stablize training process)</p>
<blockquote>
<p>如果硬要说是防止过拟合，可以这样理解：BN每次的mini-batch的数据都不一样，但是每次的mini-batch的数据都会对moving mean和moving variance产生作用，可以认为是引入了噪声，这就可以认为是进行了data augmentation，而data augmentation被认为是防止过拟合的一种方法。因此，可以认为用BN可以防止过拟合。</p>
</blockquote>
</li>
</ul>
<h4 id="Gradient-Descent-and-Backpropagation"><a href="#Gradient-Descent-and-Backpropagation" class="headerlink" title="Gradient Descent and Backpropagation"></a>Gradient Descent and Backpropagation</h4><h5 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h5><p><a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener">Gradient descent</a> is a very important idea in deep learning and machine learning, which boosts the development of AI. Of course, some researches without gradient descent were proposed, such as Newton Method, however, GD is still mainstream algorithm in training models. </p>
<blockquote>
<p><strong>Gradient descent</strong> is a <a href="https://en.wikipedia.org/wiki/Category:First_order_methods" target="_blank" rel="noopener">first-order</a> <a href="https://en.wikipedia.org/wiki/Iterative_algorithm" target="_blank" rel="noopener">iterative</a> <a href="https://en.wikipedia.org/wiki/Mathematical_optimization" target="_blank" rel="noopener">optimization</a> <a href="https://en.wikipedia.org/wiki/Algorithm" target="_blank" rel="noopener">algorithm</a> for finding the minimum of a function. To find a <a href="https://en.wikipedia.org/wiki/Local_minimum" target="_blank" rel="noopener">local minimum</a> of a function using gradient descent, one takes steps proportional to the <em>negative</em> of the <a href="https://en.wikipedia.org/wiki/Gradient" target="_blank" rel="noopener">gradient</a> (or approximate gradient) of the function at the current point. </p>
</blockquote>
<p>What GD do is updating parameters of model with gradient so that make loss descent.</p>
<p><img src="https://hackernoon.com/hn-images/0*rBQI7uBhBKE8KT-X.png" alt="Gradient Descent"> </p>
<p><img src="https://hackernoon.com/hn-images/0*8yzvd7QZLn5T1XWg.jpg" alt></p>
<h5 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h5><p><a href="https://skymind.ai/wiki/backpropagation" target="_blank" rel="noopener">Backpropagation</a> is an application of chain rule.</p>
<blockquote>
<p>A neural network propagates the signal of the input data forward through its parameters towards the moment of decision, and then <em>backpropagates</em> information about the error, in reverse through the network, so that it can alter the parameters. This happens step by step:</p>
<ul>
<li>The network makes a guess about data, using its parameters</li>
<li>The network’s is measured with a loss function</li>
<li>The error is backpropagated to adjust the wrong-headed parameters</li>
</ul>
</blockquote>
<p>See this <a href="https://brilliant.org/wiki/backpropagation/" target="_blank" rel="noopener">post1</a> and <a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">post2</a> for more details.</p>
<p><img src="https://pic2.zhimg.com/v2-8e30a45198d332dae959c57e04fdc267_r.jpg" alt="backpropagation"></p>
<p><img src="https://pic3.zhimg.com/v2-c96e284f4ab319a02fdd762367b58774_r.jpg" alt></p>
<h4 id="Gradient-vanishing-and-Gradient-exploding"><a href="#Gradient-vanishing-and-Gradient-exploding" class="headerlink" title="Gradient vanishing and Gradient exploding"></a>Gradient vanishing and Gradient exploding</h4><h5 id="Gradient-Vanishing"><a href="#Gradient-Vanishing" class="headerlink" title="Gradient Vanishing"></a>Gradient Vanishing</h5><p>As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train. Certain activation functions, like the sigmoid function, squishes a large input space into a small input space between 0 and 1. Therefore, a large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small.</p>
<p><img src="https://miro.medium.com/max/4384/1*6A3A_rt4YmumHusvTvVTxw.png" alt="sigmoid and its gradients"></p>
<p>What’s worse, we have known that neural networks’ training is based on back propagation and BP is based on chain rule, so if these values in BP are all small, such as 0.01, gradient of the first layer may be ${0.01}^n$ which is approaching to 0. According to Gradient Descent, the NNs won’t be effectively trained due to 0 gradient.</p>
<p>Solutions: </p>
<ul>
<li><p>Other activation function, such as ReLU </p>
<p>Rectifiers such as <strong>ReLU</strong> suffer less from Vanishing Gradient problem, because they only saturate in one direction, which maps $x$ into $max(0,x)$.</p>
<p>However, there still dead ReLU problem, because negative value will be mapped to zero so that some neurons won’t be activated. We can use small learning rate to partially solve this problem. Leaky ReLU and ELU are also solutions.</p>
</li>
<li><p>LSTM </p>
<p>Long Short Term Memory Networks are generally used to tackle Vanishing Gradient problem when you are working on RNN. LTSMs help to solve long term dependencies and can memorize previous data easily.</p>
</li>
<li><p><del>Residual networks</del>(according to paper of ResNet, it ‘s not for solving gradient vanishing)</p>
<p><img src="https://miro.medium.com/max/770/1*mxJ5gBvZnYPVo0ISZE5XkA.png" alt="skip connection"></p>
</li>
<li><p>Batch Normalization</p>
<p><img src="https://miro.medium.com/max/1634/1*XCtAytGsbhRQnu-x7Ynr0Q.png" alt></p>
<p>The nature of <code>Batch Normalization</code> is stablizing networks.</p>
<p>Batch Normalization can normalize all data into same distribution (zero mean, one standard deviation Gaussian). From above image we can see that when $|x|$ is too large the gradient of output of sigmoid is approaching to zero. BN normalizes the input $|x|$ so that most of it can fall into center area (green region), which means it pull data from saturated region to non-saturated region.</p>
<p>Let see BN from another perspective. For example, there is $f_2=f_1(w^T*x+b)$ in forward propagation, so  we can get $\frac{\partial f_2}{\partial x}=\frac{\partial f_2}{\partial f_1}w$ in back propagation and the value of w can lead to gradient vanishing problem. BN normalizes output of each layer into same mean and variance, which removes the effect of w’s enlarging and reducing.</p>
</li>
</ul>
<h5 id="Gradient-Exploding"><a href="#Gradient-Exploding" class="headerlink" title="Gradient Exploding"></a>Gradient Exploding</h5><blockquote>
<p>Exploding gradients are a problem when large error gradients accumulate and result in very large updates to neural network model weights during training. Gradients are used during training to update the network weights, but when the typically this process works best when these updates are small and controlled. When the magnitudes of the gradients accumulate,  an unstable network is likely to occur, which can cause poor prediction results or even a model that reports nothing useful what so ever. </p>
<p>Exploding gradients can cause problems in the training of artificial neural networks. When there are exploding gradients, an unstable network can result and the learning cannot be completed. The values of the weights can also become so large as to overflow and result in something called NaN values. NaN values, which stands for not a number, are values that represent an undefined or unrepresentable values. It is useful to know how to identify exploding gradients in order to correct the training.  - <a href="https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem" target="_blank" rel="noopener">Deep AI</a></p>
</blockquote>
<p>Solutions:</p>
<ul>
<li><p>Gradient clipping</p>
<p>Gradient clipping places a predefined threshold on the gradients to prevent it from getting too large, and by doing this it doesn’t change the direction of the gradients it only change its length.</p>
</li>
<li><p>LSTM</p>
</li>
<li><p>Batch Normalization</p>
</li>
<li><p>Regularization</p>
<p>i.e. L2 regularization. $Loss=(y-W^Tx)^2+\alpha ||W||^2$. When exploding happens, norm of weights will be very large, but it can be partially limited by regularization term. What’s more, we know that regularization can make weights small. </p>
</li>
</ul>
<h4 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h4><p>The aim of using activation function is adding non-linearity into our models, which can boost their representation ability and solve problems that linear models fail to.</p>
<p><strong>Sigmoid</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-45975854a0d00d84e575851b278a88a9_hd.jpg" alt="sigmoid"></p>
<p><img src="https://pic4.zhimg.com/50/v2-c14e1eb9092aa343fdc89d5e244fac19_hd.jpg" alt="sigmoid and its gradients"></p>
<p><strong>tanh</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-3528e66d0e12b35f778fe0ed21d2ced2_hd.jpg" alt="tanh and its gradients"></p>
<p><strong>ReLU</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-f6fb4bd3e9e4b343f87138b930792108_hd.jpg" alt="RELU"></p>
<p><img src="https://pic4.zhimg.com/50/v2-96c46de23a1f236918844dd490870c49_hd.jpg" alt="RELU and its gradients"></p>
<p>ReLU and its variants are based on $g(x;\alpha)=max(0,z)+\alpha * min(0,z)$. </p>
<p>$\alpha = 0$, it is ReLU whose gradient is computed very fast because it is either 0 or 1;</p>
<p>$\alpha=-1$, it is absolute function $g(z)=|z|$; </p>
<p>$\alpha$ is a small value, i.e. 0.01, it is <strong>Leaky ReLU</strong>;</p>
<p>$\alpha$ is a trainable parameter, it is <strong>PReLU (parametric ReLU)</strong>.</p>
<p>ReLU 会造成的低维数据的坍塌（collapse）。顾名思义，即是说，低维度的 feature 在通过 ReLU 的时候，这个 feature 会像塌方了一样，有一部分被毁掉了，或者说失去了。能恢复吗？能，但是基本无法百分百还原了。ReLU 这个东西，其实就是一个滤波器，只不过这个滤波器的作用域不是信号处理中的频域，而是特征域。那么滤波器又有什么作用呢？维度压缩，俗话说就是降维啦：如果我们有 m 个 feature 被送入 ReLU 层，过滤剩下 n 个（n&lt;m），这不就是相当于对 feature 的维度进行了压缩，使其从 m 维变为 n 维嘛。那么，为什么低维数据流经非线性激活层会发生坍塌（信息丢失），而高维数据就不会呢？维度低的 feature，分布到 ReLU 的激活带上的概率小，因此经过后信息丢失严重，甚至可能完全丢失。而维度高的 feature，分布到 ReLU 的激活带上的概率大，虽然可能也会有信息的部分丢失，但是无伤大雅，大部分的信息仍然得以保留。所谓留得青山在，不愁没柴烧嘛。更何况被 ReLU 截杀的信息，可能只是一些无用游民（冗余信息）。当信息无法流过 ReLU 时，该神经元的输出就会变为 0。而在反向传播的过程中，ReLU 对 0 值的梯度为 0，即发生了梯度消失，这将导致神经元的权重无法再通过梯度下降法进行更新，这种现象被称为特征退化。所以这个神经元相当于死掉了，丧失了学习能力。我们说，一旦神经元的输出陷入 0 值，就无法恢复了。</p>
<p>对于一个数据，利用非线性激活层对其进行激活，其实是从该数据的信息中提取出其潜在的稀疏性，但是这种提取的结果是否正确，就要分情况讨论了。对于一个 M 维的数据，我们可以将其看成是在 M 维空间中的一个 M 维流形（manifold）。而其中的有用信息，就是在该 M 维空间中的一个子空间（子空间的维度记为 N 维，N&lt;=M）中的一个 N 维流形。非线性激活层相当于压缩了这个 M 维空间的维度（还记得前面提过的维度压缩吗？）。若是该 M 维空间中的 M 维流形本来就不含有冗余信息（M=N），那么再对其进行维度压缩，必然导致信息的丢失。而维度低的数据其实就是这么一种情况：其信息的冗余度高的可能性本来就低，如果强行对其进行非线性激活（维度压缩），则很有可能丢失掉有用信息，甚至丢失掉全部信息（输出为全 0）。与非线性激活层不同的是，线性激活层并不压缩特征空间的维度。于是，我们得到了一条使用激活层的原则：对含有冗余信息的数据使用非线性激活（如 ReLU），对不含冗余信息的数据使用线性激活（如一些线性变换）。</p>
<p>由于冗余信息和非冗余信息所携带的有用信息是一样多的，因此在设计网络时，对内存消耗大的结构最好是用在非冗余信息上。根据以上的原则设计出来的结构，聪明的你想到了什么？ResNet。不得不赞叹 Kaiming He 的天才，ResNet 这东西，描述起来固然简单，但是对它的理解每深一层，就会愈发发现它的精妙及优雅，从数学上解释起来非常简洁，非常令人信服，而且直切传统痛点。ResNet 本质上就干了一件事：<strong>降低数据中信息的冗余度</strong>。具体说来，就是对非冗余信息采用了线性激活（通过 skip connection 获得无冗余的 identity 部分），然后对冗余信息采用了非线性激活（通过 ReLU 对 identity 之外的其余部分进行信息提取 / 过滤，提取出的有用信息即是残差）。由于 identity 之外的其余部分的信息冗余度较高，因此在对其使用 ReLU 进行非线性激活时，丢失的有用信息也会较少，ReLU 层输出为 0 的可能性也会较低。这就降低了在反向传播时 ReLU 的梯度消失的概率，从而便于网络的加深，以大大地发挥深度网络的潜能。</p>
<p><strong>Softmax</strong> </p>
<p>Softmax is always used as output layer of multi-classification models, which can be naturally represented as probability distribution of $n$ different values.<br>$$<br>g(z) = \frac{e^{z_i}}{\sum_{i=0}^n e^{z_i}}<br>$$<br>Softmax can solve overflow and underflow problems. The value can’t be changed if input vectors minus or plus a scalar. Thus, a simple method is $x = x - \max_i x_i$ then do softmax.</p>
<p>An interesting Chinese post: <a href="https://zhuanlan.zhihu.com/p/45014864" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/45014864</a></p>
<p><strong>Softplus</strong></p>
<p>It is a smooth version of ReLu. However, its performance is not better than ReLu.</p>
<p><strong>Maxout</strong></p>
<p>Actually Maxout is not a fixed function, however, it’s a piecewise linear function which can fit any convex function. We can regard Maxout as an layer which is similar to pooling layer and conv layer. $k$ is a hyper-parameter in Maxout,which is like $p$ in dropout.</p>
<p>i.e. $k=5$ </p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/20160103165828920.png" alt="maxout"></p>
<p>The Maxout layer has 5 neurons so that parameters increase $k=5$ folds, and the output is $max(z_1,z_2,z_3,z_4,z_5)$.</p>
<p><strong>Radial Basis Function (RBF)</strong></p>
<p>$h_i=e^{-\frac{1}{\sigma_i ^ 2}||W_{:,i}-x||}$ It is rarely used in NNs because most x can be mapped to zero, which is difficult to optimize in training.</p>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>在一定程度上限制了梯度的波动，从而降低了参数的搜索空间，因此相当于降低了模型的复杂度，所以提高了泛化能力.</p>
<h5 id="L1-L2-Regularization"><a href="#L1-L2-Regularization" class="headerlink" title="L1/L2 Regularization"></a>L1/L2 Regularization</h5><p>L1/L2 regularization is like a priori which limits distribution of parameters, so it can reduce model complexity. Meanwhile, if one feature has a very high weight and prediction is very close to ground truth, the loss would be huge due to L1/L2 term, which avoids high weights of one or several features. </p>
<p>When priori is Laplace distribution, the regularization term is L1 from; When priori is Gaussian distribution, the regularization term is L2 term. We know that $p(w=0)$ is highest in Laplace distribution and $p(w \rightarrow 0)$ is highest in Gaussian distribution. More interesting ideas in this Chinese <a href="https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&mid=2247484454&idx=1&sn=b3404eefaa68a78b770adb092d2fb48f&chksm=fb39a12dcc4e283b132138583ecd9848aa175b4cffd4036be07a2e132cb616e18ce8fe8c9adb&token=1151312247&lang=zh_CN&scene=21#wechat_redirect" target="_blank" rel="noopener">post</a>!</p>
<p>L1 regularization is sum of absolute values of all weights, which is marked as $||w||_1=\sum|w_i|$. L1 regularization can make some $w_i$ zero which leads to sparse matrix of weights. </p>

$$
\begin{align}
L &= L_0 + \frac{\lambda}{n} \sum_{w}|w|\\
\frac{\partial L}{\partial w}  &= \frac{\partial L_0}{\partial w} + \frac{\lambda}{n}sgn(w) \\
w &:= w - \alpha*\frac{\partial L}{\partial w} \ (where \ \alpha>0)\\
  &:= w - \alpha * \frac{\partial L_0}{\partial w} - \alpha * \frac{\lambda}{n}sgn(w) \\
\end{align}
$$



<p>From above equation, when $w$ is negative, updated $w$ will be greater; when $w$ is positive, updated $w$ will be smaller. Obviously, $w$ will move to zero. L1 regularization leads weights sparsity, which is important for online learning because sparsity can reduce memory and time of prediction.</p>
<p>L2 regularization is sum of square values of all weights, which is marked as $||w||_2=\sum w_i^2$. L2 regularization can make some $w_i$ approach to zero but not zero, which can prevent models from overfitting.</p>

$$
\begin{align}
L &= L_0 + \frac{\lambda}{2n} \sum_{w}w^2\\
\frac{\partial L}{\partial w}  &= \frac{\partial L_0}{\partial w} + \frac{\lambda}{n}w \\
w &:= w - \alpha*\frac{\partial L}{\partial w} \ (where \ \alpha>0)\\
  &:= w - \alpha * \frac{\partial L_0}{\partial w} - \alpha * \frac{\lambda}{n}w \\
  &:= w(1-\alpha * \frac{\lambda}{n}) - \alpha * \frac{\partial L_0}{\partial w}
\end{align}
$$


<p>From above equation, $(1-\alpha * \frac{\lambda}{n})&lt;1$, so $w$ will be reduced dramatically.</p>
<h5 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h5><p>With deep layers of NNs, the input data distribution before each layer will change, which is “Internal Covariate Shift”. That leads to slow convergence and gradient vanishing (because the distribution will be approaching to boundary of activation function).</p>
<p>A detail Chinese post : <a href="https://zhuanlan.zhihu.com/p/43200897" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/43200897</a></p>
<p>BN solves <strong>Internal Covariate Shift</strong> and makes all input data feeding in each layer same distribution.</p>
<p>In training phase, BN uses mini-batch data to compute mean and variance of $x_i$, so it’s normalization of single neuron.</p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191115210001924.png" alt="Batch Normalization"></p>
<p>Re-sacling invariant: normalization doesn’t change outputs of activation function.</p>
<blockquote>
<p>为了解决神经元输入分布偏移的情况，对每个神经元的带权输入（一个mini-batch上）进行标准化，把发生偏移的带权输入拉回到同一分布上。首先这样做解决了上述中第一个分布发生变化的问题，使得每个神经元都会适当的激活，同时标准化使得带权输入分布在0附近更容易激活神经元解决梯度消失问题。但只做标准化容易把之前学习到的特征丢弃掉，这时候就引入了伸缩因子和平移因子（这两个参数是需要学习的），相当于对标准化后的带权输入又做了个线性变换。这样使得所有神经元带权输入都有一个同一的分布，但又保持了相对差异。</p>
</blockquote>
<p>这就是Batch Normalization强大的地方，如果只做normalize在某些情况下会出现问题，比如对象是Sigmoid函数的output，而且output是分布在Sigmoid函数的两侧，normalize会强制把output分布在Sigmoid函数的中间的非饱和区域，这样会导致这层网络所学习到的特征分布被normalize破坏。而上面算法的最后一步，scale and shift可以令零均值单位方差的分布（normalize之后的分布）通过调节gamma和beta变成任意更好的分布（对于喂给下一层网络来说）。因为这个gamma和beta是在训练过程中可以学习得到参数。最极端的情况就是当gamma = sqrt(var(x)) 和 beta = mean(x)的时候，就是题主所说的“打回原形”了，这种原来的论文中，Sergey说的是至少能够使特征分布回到normalize之前的分布，并不是每一层学习到的gamma和beta都会抵消之前的normalize的操作。我的理解是完整的BN通过normalize和scale &amp; shift两步的操作提供更高的flexibility，对于每层的output既可以是零均值单位方差的分布，也可以是分布于Sigmoid两端饱和区域的分布，或者其他任意的分布。</p>
<p>More normalization: Layer Normalization/Group Normalization/Weight Normalization/Cosine Normalization/ $SELU$<sup><a href="https://arxiv.org/abs/1706.02515" target="_blank" rel="noopener">1</a>  <a href="https://zhuanlan.zhihu.com/p/27362891" target="_blank" rel="noopener">2</a></sup></p>
<p><a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">Why is BN so powerful?</a></p>
<p>那么BN有效的真正原因到底是什么呢？这还要从深度网络的损失曲面（Loss Surface）说起，在深度网络叠加大量非线性函数方式来解决非凸复杂问题时，损失曲面形态异常复杂，大量空间坑坑洼洼相当不平整，也有很多空间是由平坦的大量充满鞍点的曲面构成，训练过程就是利用SGD在这个复杂平面上一步一步游走，期望找到全局最小值，也就是曲面里最深的那个坑。所以在SGD寻优时，在如此复杂曲面上寻找全局最小值而不是落入局部最小值或者被困在鞍点动弹不得，可想而知难度有多高。</p>
<p>有了损失曲面的基本概念，我们回头来看为何BN是有效的。研究表明，BN真正的用处在于：通过上文所述的Normalization操作，使得网络参数重整（Reparametrize），它对于非线性非凸问题复杂的损失曲面有很好的平滑作用，参数重整后的损失曲面比未重整前的参数损失曲面平滑许多。</p>
<p>An interesting Chinese <a href="https://www.jiqizhixin.com/articles/2018-08-29-7" target="_blank" rel="noopener">post</a> of normalization!</p>
<p>Why can’t BN be used with dropout or other regularization methods?</p>
<blockquote>
<p><strong>我们从二者的基本出发点看，batch norm本质是控制相空间的体积，使得在映射过程中保证信号可以不衰减不爆炸，就是Jacobian的特征值的分布要合理，让系统处于一个接近临界点的状态；dropout的作用是保证系统在扰动下稳定或鲁棒，对输入信号或者系统参数系统结构的扰动不带来性能的显著恶化。定性的看，batch norm要保证信息流深，dropout要保证信息流阔。从这个角度看，二者匹配的就分别是深的网络和宽的网络，或者说解决的是这两类结构可能存在的缺陷，深的网络需要信息流足够深从而可以穿透网络，宽的网络需要信息流在相当的一个参数空间体积下都能有一定流量，否则好的解空间就容易被淹没或者错过。实际上二者的作用有相似和互补，但又有差异，这种差异使得二者分别和现有CNN网络的两部分结构匹配起来更合理一些，但并不是说不能混合起来用，BN对于全连接的部分也同样有效，dropout对CNN也一样有改善，甚至我觉得dropout应该持续作用于CNN部分，这种约束对于保证系统的鲁棒性防止过拟合会很有效的，对防范对抗攻击也会起作用，直观上使用了dropout的系统应该landscape更平缓一些。</strong></p>
<p>source：<a href="https://www.zhihu.com/question/294560934/answer/511857321" target="_blank" rel="noopener">https://www.zhihu.com/question/294560934/answer/511857321</a></p>
</blockquote>
<p>A Chinese post: <a href="https://zhuanlan.zhihu.com/p/61725100" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/61725100</a> </p>
<p><a href="https://arxiv.org/abs/1801.05134" target="_blank" rel="noopener">Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift</a></p>
<p>The key idea is variance shift. Dropout will randomly drop neurons, which makes variance change. However, BN is trying to keep variance stable. Obviously, that paradox leads variance shift and it will be increasingly serious with deeper and deeper layers.</p>
<p>In the other hand, dropout makes we cannot compute mean and variance due to we don’t have integrated data and some of that will be randomly dropped.</p>
<p>One solution is put Dropout behind BN and another is Gaussian Dropout. But combination of BN and Dropout is not far better than only BN.</p>
<p><strong>Advantages</strong></p>
<ul>
<li>Speed up convergence</li>
<li>Larger learning rate</li>
<li>Lower requirement of parameter initialization</li>
<li>Avoid gradient vanishing</li>
<li>Introduce random noise as regularization to get better generalization</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li><p>Poor performance on small batch size</p>
<ul>
<li>batch size 设置得较小训练出来的模型相对大batch size训练出的模型泛化能力更强，在测试集上的表现更好，而太大的batch size往往不太Work，而且泛化能力较差。但是背后是什么原因造成的，目前还未有定论，持不同看法者各持己见。</li>
<li>can’t apply on <strong>online-learning</strong>, because online models update parameters in single instance without mini-batch structure</li>
</ul>
</li>
<li><p>no application on dynamical networks, such as RNN, because length of their inputs is variant</p>
<ul>
<li>对于RNN来说，尽管其结构看上去是个静态网络，但在实际运行展开时是个动态网络结构，因为输入的Sequence序列是不定长的，这源自同一个Mini-Batch中的训练实例有长有短。对于类似RNN这种动态网络结构，BN使用起来不方便，因为要应用BN，那么RNN的每个时间步需要维护各自的统计量，而Mini-Batch中的训练实例长短不一，这意味着RNN不同时间步的隐层会看到不同数量的输入数据，而这会给BN的正确使用带来问题。假设Mini-Batch中只有个别特别长的例子，那么对较深时间步深度的RNN网络隐层来说，其统计量不方便统计而且其统计有效性也非常值得怀疑。另外，如果在推理阶段遇到长度特别长的例子，也许根本在训练阶段都无法获得深层网络的统计量。综上，在RNN这种动态网络中使用BN很不方便，而且很多改进版本的BN应用在RNN效果也一般。</li>
</ul>
</li>
<li><p>对于有些像素级图片生成任务来说，BN效果不佳.对于图片分类等任务，只要能够找出关键特征，就能正确分类，这算是一种粗粒度的任务，在这种情形下通常BN是有积极效果的。但是对于有些输入输出都是图片的像素级别图片生成任务，比如图片风格转换等应用场景，使用BN会带来负面效果，这很可能是因为在Mini-Batch内多张无关的图片之间计算统计量，弱化了单张图片本身特有的一些细节信息。</p>
</li>
<li><p>训练时和推理时统计量不一致</p>
<ul>
<li><p>对于BN来说，采用Mini-Batch内实例来计算统计量，这在训练时没有问题，但是在模型训练好之后，在线推理的时候会有麻烦。因为在线推理或预测的时候，是单实例的，不存在Mini-Batch，所以就无法获得BN计算所需的均值和方差，一般解决方法是采用训练时刻记录的各个Mini-Batch的统计量的数学期望，以此来推算全局的均值和方差，在线推理时采用这样推导出的统计量。虽说实际使用并没大问题，但是确实存在训练和推理时刻统计量计算方法不一致的问题。</p>
</li>
<li><p>对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，这个可以通过移动平均法求得。</p>
<p>对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和beta。BN训练时为什么不用全量训练集的均值和方差呢？因为用全量训练集的均值和方差容易过拟合，对于BN，其实就是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上能够增加模型的鲁棒性，也会在一定程度上减少过拟合。也正是因此，BN一般要求将训练集完全打乱，并用一个较大的batch值，否则，一个batch的数据无法较好得代表训练集的分布，会影响模型训练的效果。</p>
</li>
</ul>
</li>
</ul>
<p>BN can be put before activation function (original paper) or behind the function (some researches show that models get better performance).</p>
<p><strong>Variants of BN</strong></p>
<ul>
<li>Layer Normalization<ul>
<li>all neurons in the same layer as statistical area</li>
</ul>
</li>
<li>Instance Normalization<ul>
<li>CNN中将同一卷积层中每个卷积核对应的输出通道单独作为自己的统计范围</li>
</ul>
</li>
<li>Group Normalization<ul>
<li>对CNN中某一层卷积层的输出或者输入通道进行分组，在分组范围内进行统计</li>
</ul>
</li>
</ul>
<h5 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h5><p>Dropout is a popular regularization in deep learning. It can be viewed as dropping partial neurons with probability $p$ in training phase and outputs of those dropped neurons are zero. <strong>It’s ensemble of some sub-nets so that can reduce overfitting and increase generalization.</strong></p>
<p>Dropout is similar with average of multiple DNNs, so it has effect of vote.</p>
<p>When some neurons in hidden layers are dropped, fully connected networks get sparsity so that reducing collaborative effect of different features. In other words, some features may be dependent on other features (nodes), so NNs can be more robust by dropout which effectively organizes that some features are available only when some specific features survive.</p>
<h4 id="Normalization-and-Standardization"><a href="#Normalization-and-Standardization" class="headerlink" title="Normalization and Standardization"></a>Normalization and Standardization</h4><p><strong>Normalization</strong></p>
<p>Mapping data into a specific interval, usually $[-1,1]$, $[0,1]$.</p>
<ul>
<li>Min-Max Normalization</li>
</ul>
<p>$$<br>x’=\frac{x-min(x)}{max(x)-min(x)}<br>$$</p>
<p><strong>Standardization</strong></p>
<p>Mapping data into normal distribution.<br>$$<br>x’=\frac{x-\mu}{\sigma}<br>$$<br>Normalization and Standardization can improve model precision(normalization) and speed up model convergence(standardization).</p>
<h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><h5 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h5><table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Positive</th>
<th align="center">Negative</th>
</tr>
</thead>
<tbody><tr>
<td align="center">True</td>
<td align="center">TP</td>
<td align="center">TN</td>
</tr>
<tr>
<td align="center">False</td>
<td align="center">FP</td>
<td align="center">FN</td>
</tr>
</tbody></table>
<h5 id="Accuracy-Precision-Recall-F1"><a href="#Accuracy-Precision-Recall-F1" class="headerlink" title="Accuracy / Precision / Recall / F1"></a>Accuracy / Precision / Recall / F1</h5><p>$ACC=\frac{TP+TN}{TP+TN+FP+FN}$</p>
<p>$Precision=\frac{TP}{TP+FP}$</p>
<p>$Recall=\frac{TP}{TP+FN}$</p>
<p>$\frac{2}{F1}=\frac{1}{P}+\frac{1}{R}$</p>
<h5 id="AUC-Area-Under-the-Receiver-Operating-Characteristics"><a href="#AUC-Area-Under-the-Receiver-Operating-Characteristics" class="headerlink" title="AUC (Area Under the Receiver Operating Characteristics)"></a>AUC (<strong>Area Under the</strong> Receiver Operating Characteristics)</h5><p><img src="https://miro.medium.com/max/722/1*pk05QGzoWhCgRiiFbz-oKQ.png" alt="AUC"></p>
<p>Randomly choosing one sample, s1, from class A and one sample, s2, from class B, then predicting them by your classifier. P1 is the probability of your model predicting s1 as class A; P2 is the probability of model predicting s2 as class B. The probability of P1 &gt; P2 is AUC.</p>
<p>它和Wilcoxon-Mann-Witney Test是等价的。这个等价关系的证明留在下篇帖子中给出。而Wilcoxon-Mann-Witney Test就是测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score。有了这个定义，我们就得到了另外一中计 算AUC的办法：得到这个概率。<br><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116013843143.png" alt></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116013910117.png" alt></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116013921388.png" alt></p>
<h4 id="Search-of-Hyperparameters"><a href="#Search-of-Hyperparameters" class="headerlink" title="Search of Hyperparameters"></a>Search of Hyperparameters</h4><ul>
<li><p>Grid Search</p>
</li>
<li><p>Random Search</p>
</li>
<li><p>Bayesian Optimization</p>
</li>
</ul>
<p>More details in this <a href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html" target="_blank" rel="noopener">post</a>!</p>
<h4 id="Information-Theory"><a href="#Information-Theory" class="headerlink" title="Information Theory"></a>Information Theory</h4><p>Self-information: $I(x)=-logP(x)$</p>
<p>Information-entropy: $H(X)=E_{X \in P}[I(x)]=-\sum_{x \in X} P(x)logP(x)$</p>
<p>Kullback-Leibler divergence: $D_P(Q)=E_{X \in P}[log\frac{P(x)}{Q(x)}]=\sum_{x \in X}P(x)log \frac{P(x)}{Q(x)}$</p>
<p>Cross-entropy: $H_P(Q)=-E_{X \in P}logQ(x)=-\sum_{x \in X}P(x)log Q(x)$</p>
<p>$H_P(Q)=H(P)+D_P(Q)$</p>
<h3 id="Cost-Function-Loss-Function-and-Objective-Function"><a href="#Cost-Function-Loss-Function-and-Objective-Function" class="headerlink" title="Cost Function, Loss Function and Objective Function"></a>Cost Function, Loss Function and Objective Function</h3><p>Loss function is defined in single sample. Cost function is defined in whole dataset. In fact, you can use both loss function and cost function because they are same. Objective function is cost function plus regularization terms.</p>
<p>$loss=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$ is also called empirical risk; $RegularizationTerm=\lambda R(f)$ is also called structural risk. Objective function is $Obj=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda R(f)$.</p>
<p>Common loss functions:</p>
<ul>
<li><p>Classification</p>
<ul>
<li><p>Log loss \ cross entropy</p>
<p>$J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y^{(i)} log \ h_\theta(x^{(i)})+(1-y^{(i)})log \ (1-h_\theta(x^{(i)}))]$</p>
</li>
<li><p>Focal loss</p>
<p><img src="https://img-blog.csdnimg.cn/20190719190148967.png" alt="Focal Loss"></p>
</li>
<li><p>KL divergence / Relative entropy</p>
<p>$L=D_{KL}(p|q)=\sum_x{p(x)}{\log \frac{p(x)}{q(x)}}$</p>
</li>
<li><p>Exponential loss (AdaBoost)</p>
<p>$L=e^{-y_if(x_i;w)}$</p>
</li>
<li><p>Hinge loss (SVM)</p>
<p>$L=max(0, 1-yf(x;w))$</p>
</li>
</ul>
</li>
<li><p>Regression</p>
<ul>
<li><p>MSE / Quadratic loss</p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/1529558773906.png" alt="MSE"></p>
</li>
<li><p>MAE</p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/1529558773392.png" alt="MAE"></p>
</li>
<li><p>Huber loss / Smooth Mean Absolute Error</p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/1529558774147.png" alt="Huber Loss"></p>
</li>
<li><p>Smooth L1 Loss</p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191117220354211.png" alt="Smooth L1 Loss"></p>
</li>
<li><p>Log cosh loss</p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/1529558774452.png" alt="Log cosh loss"></p>
</li>
<li><p>Quantile loss</p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/1529558775749.png" alt="Quantile Loss"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h3><p>During the training process, we tweak and change the parameters (weights) of our model to try and minimize that loss function, and make our predictions as correct as possible. Optimizers tie together the loss function and model parameters by updating the model in response to the output of the loss function.</p>
<p><a href="https://leovan.me/cn/2018/02/optimization-methods-for-deeplearning/" target="_blank" rel="noopener">post1</a> <a href="http://ruder.io/optimizing-gradient-descent/index.html#fn3" target="_blank" rel="noopener">post2</a></p>
<h4 id="Gradient-Descent-GD"><a href="#Gradient-Descent-GD" class="headerlink" title="Gradient Descent (GD)"></a>Gradient Descent (GD)</h4><p>Gradient Descent is a way to minimize an objective function $J(\theta)$ parameterized by a model’s parameters $\theta \in R^d$ by updating the parameters in the opposite direction of the gradient of the objective function $\triangledown_\theta J(\theta)$ w.r.t. to the parameters. The learning rate $\eta$ determines the size of the steps we take to reach a local minimum.</p>
<p>$\theta:=\theta-\eta \frac{\partial J(\theta)}{\partial \theta}=\theta-\eta {\triangledown_\theta J(\theta)}$</p>
<h5 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a>Batch Gradient Descent</h5><p>It’s also called vanilla gradient descent, which computes the gradient of the cost function for the entire training dataset: $\theta:=\theta-\eta \frac{\partial J(\theta)}{\partial \theta}$. As we need to calculate the gradients for whole dataset to update parameters once, batch gradient descent can be very slow and is intractable for datasets that don’t fit in memory. Batch gradient descent also doesn’t allow us to update models online, aka online learning.</p>
<h5 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent (SGD)"></a>Stochastic Gradient Descent (SGD)</h5><p>SGD updates a parameter for each training example: $\theta := \theta-\eta \triangledown_\theta J(\theta;x^{(i)};y^{(i)})$, so it is much faster and can be used to learn online. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily. While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD’s fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.</p>
<h5 id="Mini-Batch-Gradient-Descent"><a href="#Mini-Batch-Gradient-Descent" class="headerlink" title="Mini-Batch Gradient Descent"></a>Mini-Batch Gradient Descent</h5><p>Mini-Batch Gradient Descent is a trade-off of batch GD and SGD, which updates a parameter for every mini-batch of n training examples: </p>
<p>$\theta := \theta-\eta \triangledown_\theta J(\theta;x^{(i:i+n)};y^{(i:i+n)})$. </p>
<h5 id="Mini-Batch-Stochastic-Gradient-Descent"><a href="#Mini-Batch-Stochastic-Gradient-Descent" class="headerlink" title="Mini-Batch Stochastic Gradient Descent"></a>Mini-Batch Stochastic Gradient Descent</h5><p>It’s easy to think of this method. Mini-Batch SGD updates a parameter for one example in a mini-batch of n training examples. In fact, what we said SGD nowadays is Mini-Batch SGD which is much faster and performs very well. Mini-Batch SGD is absolutely one of most popular optimization algorithms of ML/DL, especially for large scale dataset.</p>
<h5 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h5><p>Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</p>
<p>Learning rate schedules try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics.</p>
<p>Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.</p>
<p>Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. The difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</p>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum. Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction $\gamma$ of the update vector of the past time step to the current update vector: </p>
<p>$v_t=\gamma v_{t-1} + \eta \triangledown_\theta J(\theta)$</p>
<p>$\theta := \theta - v_t$</p>
<p>The momentum term $\gamma$ is usually set to 0.9. The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>
<h4 id="Nesterov-accelerated-gradient-NAG"><a href="#Nesterov-accelerated-gradient-NAG" class="headerlink" title="Nesterov accelerated gradient (NAG)"></a>Nesterov accelerated gradient (NAG)</h4><p>as we reach the minima i.e the lowest point on the curve ,the momentum is pretty high and it doesn’t knows to slow down at that point due to the high momentum which could cause it to miss the minima entirely and continue movie up. </p>
<p>NAG is a way to give our momentum term this kind of prescience. We know that we will use our momentum term $\gamma v_{t-1}$ to move the parameters $\theta$. Computing $\theta-\gamma v_{t-1}$ thus gives us an approximation of the next position of the parameters which gives us a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters $\theta$ but w.r.t. the approximate future position of our parameters:</p>
<p>$v_t=\gamma v_{t-1} + \eta \triangledown_\theta J(\theta - \gamma v_{t-1})$</p>
<p>$\theta := \theta - v_t$</p>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>Adagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data. </p>
<p>$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\epsilon+\sum_{i=0}^t g_t^2}}\cdot g_t $</p>
<p>One of Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of 0.01 and leave it at that.</p>
<p>Adagrad’s main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.</p>
<h4 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h4><p>Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.</p>
<p>$v_t=\gamma <em>v_{t-1}+(1-\gamma)</em>g_t^2$</p>
<p>$\triangle \theta_t = \frac{\eta}{\sqrt{v_t+\epsilon}}*g_t$</p>
<p>$\eta_t=\gamma \eta_{t-1}+(1-\gamma) \triangle \theta_t^2$</p>
<p>$\theta_{t+1}=\theta_{t}-\frac{\eta_{t}}{\sqrt{v_t+\epsilon}}*g_t$</p>
<h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p>RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad’s radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta that we derived above:</p>
<p>$v_t=\gamma <em>v_{t-1}+(1-\gamma)</em>g_t^2$</p>
<p>$\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{v_t+\epsilon}}*g_t$</p>
<p>RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests $\gamma$ to be set to 0.9, while a good default value for the learning rate $\eta$ is 0.001.</p>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients $v_t$ like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface.</p>
<p>$m_t=\beta_1 m_{t-1} + (1-\beta_1)g_t$</p>
<p>$v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2$</p>
<p>$\hat{m_t}=\frac{m_t}{1-\beta_1^t}$</p>
<p>$\hat{v_t}=\frac{v_t}{1-\beta_2^t}$</p>
<p>$\theta_{t+1}=\theta_t - \frac{\eta}{\sqrt{\hat{v_t}}+\epsilon}\hat{m_t}=\theta_t - \frac{\eta}{\sqrt{\hat{v_t}}+\epsilon}(\beta_1\hat{m_{t-1}}+\frac{(1-\beta_1)g_t}{1-\beta_1^t})$</p>
<p>The authors propose default values of 0.9 for $\beta_1$, 0.999 for $\beta_2$, and $10^{-8}$ for $\epsilon$. Adam can be viewed as a combination of RMSprop and momentum.</p>
<p>Adam is good, but there are still some problems. Some researches are <a href="https://arxiv.org/abs/1712.07628" target="_blank" rel="noopener">Improving Generalization Performance by Switching from Adam to SGD</a>.</p>
<h4 id="AdaMax"><a href="#AdaMax" class="headerlink" title="AdaMax"></a>AdaMax</h4><p>$u_t=max(\beta_2 \cdot v_{t-1}, |g_t|)$</p>
<p>$\theta_{t+1}=\theta_t - \frac{\eta}{u_t} \hat{m_t}$</p>
<p>Good default values are given $\eta=0.002$, $\beta_1=0.9$, and $\beta_2=0.999$. </p>
<h4 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h4><p>Nadam (Nesterov-accelerated Adaptive Moment Estimation) thus combines Adam and NAG. Nadam = Adam + Nesterov.</p>
<p>$m_t=\beta_1 m_{t-1} + (1-\beta_1)g_t$</p>
<p>$v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2$</p>
<p>$\hat{m_t}=\frac{m_t}{1-\beta_1^t}$</p>
<p>$\hat{v_t}=\frac{v_t}{1-\beta_2^t}$</p>
<p>$\theta_{t+1}=\theta_t - \frac{\eta}{\sqrt{\hat{v_t}}+\epsilon}(\beta_1\hat{m_t}+\frac{(1-\beta_1)g_t}{1-\beta_1^t})$</p>
<h4 id="AMSGrad"><a href="#AMSGrad" class="headerlink" title="AMSGrad"></a>AMSGrad</h4><p>The exponential moving average of past squared gradients as a reason for the poor generalization behaviour of adaptive learning rate methods. Recall that the introduction of the exponential average was well-motivated: It should prevent the learning rates to become infinitesimally small as training progresses, the key flaw of the Adagrad algorithm. However, this short-term memory of the gradients becomes an obstacle in other scenarios.</p>
<p>$m_t=\beta_1 m_{t-1} + (1-\beta_1)g_t$</p>
<p>$v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2$</p>
<p>$\hat{v_t}=max(\hat{v}_{t-1},v_t)$</p>
<p>$\theta_{t+1}=\theta_t - \frac{\eta}{\sqrt{\hat{v_t}}+\epsilon}m_t$</p>
<h4 id="Follow-the-Regularized-Leader-FTRL"><a href="#Follow-the-Regularized-Leader-FTRL" class="headerlink" title="Follow the Regularized Leader (FTRL)"></a>Follow the Regularized Leader (FTRL)</h4><p><a href="http://vividfree.github.io/机器学习/2015/12/05/understanding-FTRL-algorithm" target="_blank" rel="noopener">http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/05/understanding-FTRL-algorithm</a></p>
<p>在工业界，越来越多的业务需要大规模机器学习，不单参与训练的数据量大，模型特征量的规模也大。例如点击率预估，训练数据量在TB量级，特征量在亿这个量级，业内常用LR（Logistic Regression）和FM（Factorization Machines）为点击率预估建模。对LR、FM这类模型的参数学习，传统的学习算法是batch learning算法，它无法有效地处理大规模的数据集，也无法有效地处理大规模的在线数据流。这时，有效且高效的online learning算法显得尤为重要。</p>
<p>SGD算法是常用的online learning算法，它能学习出不错的模型，但学出的模型不是稀疏的。为此，学术界和工业界都在研究这样一种online learning算法，它能学习出有效的且稀疏的模型。FTRL（Follow the Regularized Leader）算法正是这样一种算法，它由Google的H. Brendan McMahan在2010年提出的，作者后来在2011年发表了一篇关于FTRL和AOGD、FOBOS、RDA比较的论文，2013年又和Gary Holt, D. Sculley, Michael Young等人发表了一篇关于FTRL工程化实现的论文。如论文的内容所述，FTRL算法融合了RDA算法能产生稀疏模型的特性和SGD算法能产生更有效模型的特性。它在处理诸如LR之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色，国内各大互联网公司都已将该算法应用到实际产品中。</p>
<img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116190711218.png" style="zoom:150%;">

<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116190725165.png" alt></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116190735241.png" alt></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116190745096.png" alt></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116190820127.png" alt></p>
<p>Equation above can be equation below which is SGD equation:</p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116190900745.png" alt></p>
<p>In conclusion, in equation of FTRL left side is equal to SGD and right side is L1 norm which can make sparse solutions.</p>
<p>SGD引入L1实际上不太容易产生稀疏解，但是按照上面的推导来看ftrl就是在SGD的基础上引入了L1.在GD算法下，L1正则化通常能得到更加稀疏的解；可是在SGD算法下模型迭代并不是沿着全局梯度下降，而是沿着某个样本的梯度进行下降，这样即使是L1正则也不一定能得到稀疏解。在后面的优化算法中，稀疏性是一个主要追求的目标。</p>
<p><a href="https://courses.cs.washington.edu/courses/cse599s/12sp/scribes/Lecture8.pdf" target="_blank" rel="noopener">https://courses.cs.washington.edu/courses/cse599s/12sp/scribes/Lecture8.pdf</a></p>
<p><a href="http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf" target="_blank" rel="noopener">http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf</a></p>
<p>不论怎样，简单截断、TG、FOBOS都还是建立在SGD的基础之上的，属于梯度下降类型的方法，这类型方法的优点就是精度比较高，并且TG、FOBOS也都能在稀疏性上得到提升。但是有些其它类型的算法，例如RDA从另一个方面来求解Online Optimization并且更有效地提升了特征权重的稀疏性。RDA（Regularized Dual Averaging）是微软十年的研究成果。</p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116191751785.png" alt="FOBOS, RDA and FTRL"></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116191804483.png" alt></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116191913836.png" alt></p>
<p>有效稀疏：所谓有效稀疏，就是要避免某个权重还没训练充分就被稀疏成0，这时候，累计梯度的作用就出来了，这保证了某权重被训练到一定程度了，才开始稀疏。SGD为啥不行？稀疏手段还是次要的，主要还是没有做有效稀疏，而有效稀疏的关键是累计梯度。</p>
<h4 id="Lookahead"><a href="#Lookahead" class="headerlink" title="Lookahead"></a>Lookahead</h4><p><a href="https://arxiv.org/abs/1907.08610v1" target="_blank" rel="noopener">https://arxiv.org/abs/1907.08610v1</a></p>
<h4 id="Newton’s-method-and-Quasi-Newton-methods"><a href="#Newton’s-method-and-Quasi-Newton-methods" class="headerlink" title="Newton’s method and Quasi-Newton methods"></a>Newton’s method and Quasi-Newton methods</h4><h5 id="Newton’s-Method"><a href="#Newton’s-Method" class="headerlink" title="Newton’s Method"></a>Newton’s Method</h5><p><a href="https://en.wikipedia.org/wiki/Newton' target=" _blank" rel="noopener" s_method">Newton’s method</a> finds next position by tangent line of current position.</p>
<p>$x_{n+1}=x_{n}-\frac{f(x_{n})}{f’(x_n)}$</p>
<p>Newton’s method is faster than gradient descent because it is second convergence. However, Newton’s method needs to compute inverse matrix of Hessian matrix of objective function, so the computation is very complex.</p>
<p>$\theta := \theta - H^{-1}J(\theta)$</p>
<h5 id="Quasi-Newton-methods"><a href="#Quasi-Newton-methods" class="headerlink" title="Quasi-Newton methods"></a>Quasi-Newton methods</h5><p>Fitting inverse matrix of Hessian matrix by positive matrix</p>
<ul>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
<h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h3><h4 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h4><p>The most basic regression model, $y=w^Tx+b$.</p>
<h4 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h4><p>LR is initially proposed for binary classification, i.e. $y_1$ and $y_2$.</p>

$$
\begin{align}
p(y=y_1|x) &= \frac{p(x|y_1)p(y_1)}{p(x)} \\
&= \frac{p(x|y_1)p(y_1)}{p(x|y_1)p(y_1)+p(x|y_2)p(y_2)} \\
&= \frac{1}{1+\frac{p(x|y_2)p(y_2)}{p(x|y_1)p(y_1)}} \\
\end{align}
\\ Assumed \ ln\frac{p(x|y_1)p(y_1)}{p(x|y_2)p(y_2)}=a, \ so \ p(y=y_1|x)=\frac{1}{1+e^{-a}}
$$


<p>We know that LR formulation is as following:</p>
<p>$h_{\theta}=\frac{1}{1+e^{-z}}$</p>
<p>$z=\theta^Tx+b$</p>
<p>The likelihood function is $L(\theta)=\prod_{i=1}^N h_\theta(x)^{y_i}(1-h_\theta(x))^{1-y_i}$.</p>
<p>Thus, loss function is $J(\theta)=-L(\theta)$.</p>
<h4 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h4><p><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" target="_blank" rel="noopener">KNN</a> is a non-parametric method used for classification and regression. The lower the k is, the more sensitive the model is.</p>
<h4 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h4><p>The key idea of SVM is maximizing margins. Margin is the distance between hyper-plane and support vectors. Support vectors are those nearest points to hyper-plane.</p>
<p>Hyper-plane: $w^Tx+b=0$</p>
<p>Functional margin: $\hat{\gamma} = |w^Tx+b| = y(w^Tx+b)$</p>
<p>Minimum of functional margin in all samples: $\hat{\gamma}=min \hat{\gamma_i}$</p>
<p>Geometric margin: $\gamma = \frac{\hat{\gamma}}{||w||}$</p>
<p>In SVM, we hope maximize the margin, so</p>

$$
\max_{w,b} \ \gamma \\
s.t. y_i(\frac{w^Tx+b}{||w||}) \ge \gamma, i=1,2,\cdots,N \\
\Rightarrow \max_{w,b} \frac{\hat{\gamma}}{||w||} \\
s.t. y_i(w^Tx_i+b) \ge \hat{\gamma}, i=1,2,\cdots,N
$$


<p>Functional margin is minimum distance between sample point and hyper-plane. If making functional margin be $1$, distance between other points and hyper-plane is more than $1$. So,</p>

$$
\max_{w,b} \frac{1}{||w||} \\
\Rightarrow \min_{w,b} \frac{1}{2}||w||^2 \\
s.t. y_i(w^Tx_i+b)-1 \ge 0, i=1,2,\cdots,N \\
L(w,b,\alpha) = \frac{1}{2}||w||^2+\alpha_i \sum_{i=1}^N[-y_i(w^Tx+b)+1]=\frac{1}{2}||w||^2-\sum_{i=1}^N \alpha_iy_i(w^Tx_i+b)+\sum_{i=1}^N \alpha_i \\
\because \alpha_i \ge 0 \\
\therefore \max_\alpha L(w,b,\alpha)=\frac{1}{2}||w||^2 \\
\therefore \min_{w,b} \frac{1}{2}||w||^2 \Rightarrow \min_{w,b}\max_\alpha L(w,b,a) \\
According \ to \ Lagrange \ duality \ problem, 
\min_{w,b}\max_\alpha L(w,b,\alpha) \Rightarrow \max_\alpha \min_{w,b} L(w,b,\alpha)\\
\min_{w,b}L(w,b,\alpha) \Rightarrow \triangledown_wL(w,b,\alpha) = w-\sum_{i=1}^N\alpha_iy_ix_i = 0 \ and \ \triangledown_bL(w,b,\alpha)=\sum_{i=1}^N\alpha_iy_i=0 \\
w = \sum_{i=1}^N \alpha_iy_ix_i \\
\sum_{i=1}^N\alpha_iy_i=0 \\
\therefore \min_{w,b}L(w,b,\alpha) = -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N \alpha_i \\
Then, \ \max_\alpha -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N \alpha_i, s.t. \sum_{i=1}^N \alpha_iy_i=0 \ and \ \alpha_i \ge 0\\
\Rightarrow \min_\alpha \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^N \alpha_i, s.t. \sum_{i=1}^N \alpha_iy_i=0 \ and \ \alpha_i \ge 0 \\
Computing \ \alpha^* \ by \ SMO \ algorithm \\
b^* = y_j - \sum_{i=1}^N \alpha_i^*y_ix_i^Tx_j \\
hyper-plane \Rightarrow \sum_{i=1}^N \alpha_i^*y_ix^Tx_i + b^* = 0 \\
f(x) = sign(\sum_{i=1}^N \alpha_i^*y_ix^Tx_i + b^*)
$$


<p>Above content is soft margin SVM which is used in linear classification. However, there’re usually some outliers in dataset, so the problem is about near-linear classification. We should use hard margin SVM which introduces a slack variable $\xi$ ($\xi \ge 0$), then constraint condition becomes $y_i(w^Tx_i+b) \ge 1-\xi_i$. The objective function is then modified to $\frac{1}{2}||w||^2+C \sum_{i=1}^N \xi_i$, where $C &gt; 0$ (penalty parameter). When $C$ is large, the penalty of misclassification becomes large; when $C$ is small, the penalty becomes small. $C$ Is like $\lambda$ in regularization and controls margins of model and importance of misclassified points. Hard margin SVM is computed in same way of soft margin SVM.</p>
<p>However, there are still some non-linear classification problem. It’s time to introduce kernel trick, which maybe one of coolest invitations in ML. </p>
<p>Kernel: linear kernel, polynomial kernel, Gaussian kernel, etc</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&mid=2247484795&idx=1&sn=895b56d7ad79e4dcbf73cea448167a55&chksm=fb39a070cc4e29661f72e2ccb78081dffff2c581963026a12b961276d6d9fe8a2bd8ce61cd24&token=1838236212&lang=zh_CN&scene=21#wechat_redirect" target="_blank" rel="noopener">SMO</a></p>
<h4 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h4><p><a href="https://en.wikipedia.org/wiki/Decision_tree" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Decision_tree</a></p>
<h5 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h5><p>Information gain is the metric of choosing and split features.</p>
<p>$Ent(D)=-\sum_{k=1}^{y}p_k \log_2p_k$</p>
<p>$Gain(D,a)=Ent(D)-\sum_{v=1}^V \frac{|D^v|}{|D|}Ent(D^v)$ </p>
<p>ID3 chooses highest $Gain(D,a)$. </p>
<h5 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h5><p>$GainRatio(D,a)=\frac{Gain(D,a)}{IV(a)}$</p>
<p>$IV(a)=-\sum_{v=1}^V \frac{|D^v|}{|D|} \log_2 \frac{|D^v|}{|D|}$</p>
<h5 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h5><p>$Gini(D)=1-\sum_{k=1}^Np_k^2$</p>
<p>$Gini_a=\sum_{v=1}^V \frac{|D^v|}{|D|}Gini(D^v)$</p>
<p>CART chooses lowest $Gini_a$.</p>
<h4 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h4><p>RF is a bagging algorithm (bootstrapping) which is based on decision tree. RF concludes some trees and each tree can give prediction. Every model in RF has similar bias and variance, </p>

$$
E[\frac{\sum X_i}{K}] = E[X_i] \\
\frac{Var(\sum X_i)}{K} \le Var(\frac{\sum X_i}{K}) \le Var(X_i) \\
Equation \ is \ valid \ when \ trees \ are \ independent \ or \ same.
$$


<p>We can find that RF can reduce variance, so RF is an effective way to solve overfitting problem. What’s more, the relavance of trees is associated with that ability, thus reducing the relavance is important. Fortunately, input data of each tree is subsampled in whole dataset, so we can try to choose different features with low overlap. Obviously, number of features is an important hyper-parameters to RF.</p>
<p>Important parameters: <strong>n_estimators</strong>; <strong>max_features</strong>; max_depth; min_samples_leaf; min_samples_split; max_leaf_nodes; min_samples_leaf</p>
<h4 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h4><p>AdaBoost is belonging to Boosting algorithm which literately combines weak learners to get the final strong learner. In Boosting, data are all given weights and misclassified data can get higher weights responding to higher importance. Meanwhile, those series of learners also have weights: low weights are given to high error learners, and high weights are given to low error leaners.</p>
<p>It’s time to talk about AdaBoost. It can be used in both classification and regression.</p>
<p>Assuming that is a binary classification problem with {-1, 1} as output and $k_{th}$ learner is denoted to $G_k(x)$.</p>

$$
e_k = P(G_k(x_i) \neq y_i) = \sum_{i=1}^m w_{ki}I(G_k(x_i) \neq y_i) \\
\alpha_k = \frac{1}{2} \log \frac{1-e_k}{e_k} \\ large \ \alpha \ is \ followed \ by \ small \ e \\
w_{k+1,i} = \frac{w_{k,i}}{Z_i} \exp (-\alpha_i G_k(x_i) y_i) \\
Z_k = \sum_{i=1}^m w_{k,i} \exp (-\alpha_i G_k(x_i)y_i) \\
G(x) = sign(\sum_{m=1}^K \alpha_m G_m(x))
$$


<p>If it is misclassification, $G_k(x_i)y_i=-1$, $w_{k+1,i} &gt; w_{k,i}$ which means that weights of misclassification get higher; otherwise,  $G_k(x_i)y_i=1$, $w_{k+1,i}&lt;w_{k,i}$.</p>
<p>Can we do regression by AdaBoost? Of course!</p>

$$
E_k = \max |y_i - G_k(x_i)|, i = 1,2,\cdots,m \\
e_{k,i} = \begin{cases}
\frac{|y_i - G_k(x_i)|}{E_k} & Linear \ Error \\
\frac{(y_i - G_k(x_i))^2}{E_k^2} & MSE \\
1 - \exp (\frac{-|y_i-G_k(x_i)|}{E_k}) & Exponential \ Error
\end{cases} \\
e_k = \sum_{i=1}^m w_{k,i} e_{k,i} \\
\alpha_k = \frac{e_k}{1-e_k} \\
w_{k+1,i} = \frac{w_{k,i}}{Z_k} \alpha_k^{1-e_{k,i}} \\
Z_k = \sum_{i=1}^m w_{k,i} \alpha_k^{1-e_{k,i}} \\
f(x)=\sum_{k=1}^K(\ln \frac{1}{\alpha_k})g(x) \\
g(x) \ is \ median \ of \ all \ \alpha_kG_k(x)
$$



<h4 id="Gradient-Boost-Decision-Tree-GBDT"><a href="#Gradient-Boost-Decision-Tree-GBDT" class="headerlink" title="Gradient Boost Decision Tree (GBDT)"></a>Gradient Boost Decision Tree (GBDT)</h4><p>In GBDT, we are using minus gradient of loss function to fit residual.</p>

$$
r_{m,i} = - [\frac{\partial L(y_i,f(x_i)}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)} \\
\Rightarrow R_{m,j} \\
T_{m,j}=argmin_T \sum_{x_i \in R_{m,j}} L(y_i,f_{m-1}(x_i)+T) \\
f_m(x) = f_{m-1}(x)+\sum_{j=1}^J c_{m,j}I(x \in R_{m,j})) \\
\hat{f(x)} = f_M(x)=f_0(x)+\sum_{m=1}^M \sum_{j=1}^J T_{m,j}I(x \in R_{m,j})
$$



<h4 id="XGBOOST"><a href="#XGBOOST" class="headerlink" title="XGBOOST"></a>XGBOOST</h4><p>$\Omega(f_t)= \gamma T + \frac{1}{2}\lambda \sum_{j=1}^Tw_j^2$</p>
<p>$T$ is number of leaf nodes; $w$ is leaf node scores.</p>

$$
\begin{align}
Obj^{(t)} &= \sum_{i=1}^n L(y_i, \hat{y_i}^{(t)})+\sum_{k=1}^t \Omega(f_k) \\
&= \sum_{i=1}^n L(y_i, \hat{y_i}^{(t-1)}+f_t(x_i)) + \sum_{k=1}^{t-1} \Omega(f_k) + \Omega(f_t) \\
&= \ \sum_{i=1}^n L(y_i, \hat{y_i}^{(t-1)}+f_t(x_i)) + \Omega(f_t) + Constant \\
\because & f(x_0+\triangle x) \approx f(x_0)+f'(x_0) \cdot \triangle x + \frac{f''(x_0)}{2} \cdot (\triangle x)^2 \\
\therefore &Obj^{(t)} \approx \sum_{i=1}^n [L(y_i, \hat{y_i}^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t(x_i)^2] + \Omega(f_t) + Constant \\
\because &L(y_i, \hat{y_i}^{(t-1)}) \ is \ constant \\
\therefore & Obj^{(t)} = \sum_{i=1}^n [g_if_t(x_i)+\frac{1}{2}h_if_t(x_i)^2] + \Omega(f_t) + Constant \\
&g_i =\partial_{\hat{y}^{(t-1)}}L(y_i, \hat{y}^{(t-1)});h_i=\partial_{\hat{y}^{(t-1)}}^2L(y_i, \hat{y}^{(t-1)}) \\
\because &\Omega(f_t)= \gamma T + \frac{1}{2}\lambda \sum_{j=1}^Tw_j^2 \\
\therefore Obj^{(t)} &= \sum_{i=1}^n[g_i \cdot w_{q(x_i)}+\frac{1}{2}h_i \cdot w_{q(x_i)}^2]+\gamma T + \frac{1}{2}\lambda \sum_{j=1}^Tw_j^2 \\
&= \sum_{j=1}^T[(\sum_{i \in I_j}g_i)w_j+\frac{1}{2}(\sum_{i \in I_j}h_i+\lambda)w_j^2]+\gamma T \\
&= \sum_{j=1}^T[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2]+\gamma T \\
\Rightarrow &w_j^*=-\frac{G_j}{H_j+\lambda} \\
\Rightarrow &Obj^*=-\frac{1}{2}\sum_{j=1}^T \frac{G_j^2}{H_j+\lambda}+\gamma T \ (scoring-function)
\end{align}
$$


<p>Scoring function measures tree structures, so the low the value, the better structure the better. Choosing best split point by scoring function to build CART.<br>$$<br>Gain = \frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]-\gamma<br>$$<br>$Gain$ represents difference between single node $obj^<em>$ and two node $obj^</em>$. Traversing all features’ splitting points to find splitting point with max $Gain$. $G_L, G_R$ Is sum of first order gradient of left nodes and right nodes. $H_L, H_R$ is sum of second order gradient of left nodes and right nodes. $\lambda$ is regularization coefficient. $\gamma$ is difficulty of splitting node. $\lambda$ and $\gamma$ define complexity of trees.</p>
<p>If $\gamma$ is too large, $Gain$ is negative, which represents not splitting; The higher the $\gamma$, the more strict the obj’s dropping.</p>
<p>Difference with GBDT:</p>
<ul>
<li>XGBoost would consider complexity of CART in building process, but GBDT wouldn’t.</li>
<li>XGBoost fits last time loss function’s second order gradient, but GBDT just considers first order gradient.</li>
<li>XGBoost can compute parallelly (not in building tree, but in feature processing)<ul>
<li>xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li>
</ul>
</li>
<li>XGBoost can deal with missing data</li>
<li>XGBoost can customize loss function if only it has second order gradient</li>
<li>XGBoost have column subsampling<ul>
<li>xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算.</li>
</ul>
</li>
</ul>
<p>Details of splitting algorithm can be seen in this <a href="https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&mid=2247485414&idx=1&sn=faa60d6b4653b9a73e6c5b1ae5f9fd22&chksm=fb39a2edcc4e2bfbe4d4b64c1d7c78b08ec6a4a07e03dd21c634add55bff1ced9693b75ecf88&token=309753865&lang=zh_CN&scene=21#wechat_redirect" target="_blank" rel="noopener">post</a>.</p>
<p>Details of tuning parameters can be seen in this <a href="https://sunjunee.github.io/2018/01/05/xgboost-tuning/" target="_blank" rel="noopener">post</a>.</p>
<h4 id="Light-GBM"><a href="#Light-GBM" class="headerlink" title="Light GBM"></a>Light GBM</h4><p> <a href="https://zhuanlan.zhihu.com/p/35155992" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35155992</a> </p>
<p>提升树是利用加模型与前向分布算法实现学习的优化过程，它有一些高效实现，如XGBoost, pGBRT，GBDT等。其中GBDT采用负梯度作为划分的指标（信息增益），XGBoost则利用到二阶导数。他们共同的不足是，计算信息增益需要扫描所有样本，从而找到最优划分点。在面对大量数据或者特征维度很高时，他们的效率和扩展性很难使人满意。微软开源的LightGBM（基于GBDT的）则很好的解决这些问题，它主要包含两个算法：</p>
<ul>
<li>GOSS（从减少样本角度）：排除大部分小梯度的样本，仅用剩下的样本计算信息增益。</li>
<li>EFB（从减少特征角度）：捆绑互斥特征，也就是他们很少同时取非零值（也就是用一个合成特征代替）。</li>
</ul>
<p>GBDT是基于决策树的集成算法，采用前向分布算法，在每次迭代中，都是通过负梯度拟合残差，从而学习一颗决策树，最耗时的步骤就是找最优划分点。一种流行的方法就是预排序，核心是在已经排好序的特征值上枚举所有可能的特征点。另一种改进则是直方图算法，他把连续特征值划分到k个桶中取，划分点则在这k个点中选取。k&lt;&lt;d，所以在内存消耗和训练速度都更佳，且在实际的数据集上表明，离散化的分裂点对最终的精度影响并不大，甚至会好一些。原因在于决策树本身就是一个弱学习器，采用Histogram算法会起到正则化的效果，有效地防止模型的过拟合。LightGBM也是基于直方图的。</p>
<p>为了减少训练数据，最直接的方法就是欠采样(down sample)，比如说过滤掉权重低于阈值的样本。SGD(随机梯度下降)采用的是在每轮迭代中选取随机子集进行训练弱分类器，AdaBoost则采用的是动态调整采样率。SGD可以应用到GBDT，但会影响精度，其他的则不能直接引入，因为GBDT中没有这种内在的权重。</p>
<p>为了减少特征，通常做的是PCA降维，但是这些方法都假设特征是冗余的，这并不一直正确。</p>
<p>一般大型数据集都是稀疏的，基于pre-sorted的GBDT可以通过忽略零值特征，从而减少训练代价。但是，基于histogram的则没有针对稀疏特性的优化方案，它只是计算累加值，不管你是0还是非0。所以，利用稀疏性的GBDT是很必要的。</p>
<p><strong>GOSS</strong></p>
<p>在AdaBoost中采用权重很好诠释了样本的重要性，GBDT没有这种权重，但是我们注意到每个数据样本的梯度可以被用来做采样的信息。也就是，如果一个样本的梯度小，那么表明这个样本已经训练好了，它的训练误差很小了，我们可以丢弃这些数据。当然，改变数据分布会造成模型的精度损失。GOSS则通过保存大梯度样本，随机选取小梯度样本，并为其弥补上一个常数权重。这样，GOSS更关注训练不足的样本，同时也不会改变原始数据太多。 </p>
<p> <img src="https://pic3.zhimg.com/80/v2-1608d8b79f3d605e111878b715254d3e_hd.jpg" alt="GOSS"></p>
<p>先根据梯度对样本进行排序，选取 a * 100% 的top样本，再从剩余数据中随机选取 b * 100% 的样本，并乘以 的系数放大。</p>
<p>以前计算特征j在d值点的信息增益是这样的：</p>
<p> <img src="https://pic2.zhimg.com/80/v2-4b9a8ede817e0e68d889707b452fd921_hd.jpg" alt></p>
<p> 现在是这样的：</p>
<p> <img src="https://pic2.zhimg.com/80/v2-ea968fb24797e33e1ed60d9178814be5_hd.jpg" alt> </p>
<p>通过证明，近似误差很好，很贴近使用所有数据的模型。这也解释了LightGBM的 leaf-wise 生成策略。</p>
<p><strong>Leaf-wise (Best-first)的决策树生长策略</strong></p>
<p>大部分决策树的学习算法通过 level(depth)-wise 策略生长树，如下图一样：</p>
<p> <img src="https://pic2.zhimg.com/80/v2-255136baf2eaeec7841db3a2e45ca5a1_hd.jpg" alt="Level-wise"></p>
<p>LightGBM 通过 leaf-wise (best-first)策略来生长树。它将选取具有最大 delta loss 的叶节点来生长。 当生长相同的 #leaf，leaf-wise 算法可以比 level-wise 算法减少更多的损失。当 #data 较小的时候，leaf-wise 可能会造成过拟合。 所以，LightGBM 可以利用额外的参数 max_depth 来限制树的深度并避免过拟合（树的生长仍然通过 leaf-wise 策略）。</p>
<p> <img src="https://pic3.zhimg.com/80/v2-3f5b87c2c32efdfd33184be1e182b492_hd.jpg" alt="Leaf-wise"> </p>
<p><strong>EFB</strong></p>
<p>高维数据一般是稀疏的，可以设计一种损失最小的特征减少方法。并且，在稀疏特征空间中，许多特征都是互斥的，也就是它们几乎不同时取非0值。因此，我们可以安全的把这些互斥特征绑到一起形成一个特征，然后基于这些特征束构建直方图，这样又可以加速了。</p>
<p>有两个问题待解决，如何判断哪些特征该绑到一起，如何构建绑定。这是NP难的。</p>
<p>首先，转换到图着色问题。G=(V, E)，把关联矩阵G的每一行看成特征，从而得到|V|个特征，互斥束就图中颜色相同的顶点。图中点就是特征，边代表两个特征不互斥，也就是特征之间的冲突。如果算法允许小的冲突，可以得到更小的特征束数量，计算效率会更高。证明发现随机污染一小部分特征值，最多影响训练精度 ，是所有束中冲突最大的。通过选取合适的，我们可以很好的在效率和精度之间寻找平衡。</p>
<p>最后，排序就按照束的度来进行。当然，更一步优化是不够造图，直接<strong>根据非零值的数量排序</strong>，这个根据度排序很像，因为更多非0值意味着更高概率的冲突。更改了排序策略，可以避免重复。</p>
<p>第二个问题，合并特征，从而降低训练复杂度，关键是我们可以确保原先特征值可以从特征束中识别出来。因为直方图存储的是特征的离散桶，而不是连续值，我们可以通过把互斥特征放到不同桶，从而构造一个特征束。这可以通过添加偏移实现。如，假设我们有2个特征在一个特征束中，原先特征A的范围为[0,10)，特征B的范围为[0,20)，我们给特征B加上一个偏移10，它就变成[10,30)，这样我们就可以执行安全的合并了，用特征束[0,30)代替特征A和B。具体算法如下。</p>
<p> <img src="https://pic1.zhimg.com/80/v2-4d7875a088e184c694474be2bec26698_hd.jpg" alt="EFB algorithm"> </p>
<p>EFB算法可以把很多特征绑到一起，形成更少的稠密特征束，这样可以避免对0特征值的无用的计算。加速计算直方图还可以用一个表记录数据的非0值。 </p>
<hr>
<p> <a href="https://www.zhihu.com/question/51644470" target="_blank" rel="noopener">https://www.zhihu.com/question/51644470</a> </p>
<p>GBDT 虽然是个强力的模型，但却有着一个致命的缺陷，不能用类似 mini batch 的方式来训练，需要对数据进行无数次的遍历。如果想要速度，就需要把数据都预加载在内存中，但这样数据就会受限于内存的大小；如果想要训练更多的数据，就要使用外存版本的决策树算法。虽然外存算法也有较多优化，SSD 也在普及，但在频繁的 IO 下，速度还是比较慢的。</p>
<p>为了能让 GBDT 高效地用上更多的数据，我们把思路转向了分布式 GBDT， 然后就有了 LightGBM。设计的思路主要是两点，1. 单个机器在不牺牲速度的情况下，尽可能多地用上更多的数据；2.多机并行的时候，通信的代价尽可能地低，并且在计算上可以做到线性加速。</p>
<p>基于这两个需求，LightGBM 选择了基于 histogram 的决策树算法。相比于另一个主流的算法 pre-sorted（如 xgboost 中的 exact 算法），histogram 在内存消耗和计算代价上都有不少优势。</p>
<ul>
<li>Pre-sorted 算法需要的内存约是训练数据的两倍(2 * #data * #features<br>* 4Bytes)，它需要用32位浮点来保存 feature value，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位的存储空间。对于 histogram 算法，则只需要(#data* #features * 1Bytes)的内存消耗，仅为 pre-sorted算法的1/8。因为 histogram 算法仅需要存储 feature<br>bin value (离散化后的数值)，不需要原始的 feature value，也不用排序，而 bin value 用 uint8_t (256 bins) 的类型一般也就足够了。</li>
<li>在计算上的优势则主要体现在“数据分割”。决策树算法有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，Histogram 算法和 pre-sorted 算法在“寻找分割点”的代价是一样的，都是O(#feature*#data)。而在“数据分割”时，pre-sorted 算法需要O(#feature*#data)，而 histogram 算法是O(#data)。因为 pre-sorted 算法的每一列特征的顺序都不一样，分割的时候需要对每个特征单独进行一次分割。Histogram算法不需要排序，所有特征共享同一个索引表，分割的时候仅需对这个索引表操作一次就可以。（更新: <strong>这一点不完全正确，pre-sorted 与 level-wise 结合的时候，其实可以共用一个索引表(row_idx_to_tree_node_idx)。然后在寻找分割点的时候，同时操作同一层的节点，省去分割的步骤。但这样做的问题是会有非常多随机访问，有很大的cache miss，速度依然很慢。</strong>）</li>
<li>另一个计算上的优势则是大幅减少了计算分割点增益的次数。对于一个特征，pre-sorted 需要对每一个不同特征值都计算一次分割增益，而 histogram 只需要计算 #bin (histogram 的横轴的数量) 次。</li>
<li>最后，在数据并行的时候，用 histgoram 可以大幅降低通信代价。用 pre-sorted 算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst 在并行的时候也使用 histogram 进行通信。</li>
</ul>
<p>当然， histogram 算法也有缺点，它不能找到很精确的分割点，训练误差没有 pre-sorted 好。但从实验结果来看， histogram 算法在测试集的误差和 pre-sorted 算法差异并不是很大，甚至有时候效果更好。实际上可能决策树对于分割点的精确程度并不太敏感，而且较“粗”的分割点也自带正则化的效果。</p>
<p>在 histogram 算法之上， LightGBM 进行进一步的优化。首先它抛弃了大多数 GBDT 工具使用的按层生长<br>(level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。 level-wise 过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，不容易过拟合。但实际上level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大(一般也是数据量最大)的一个叶子，然后分裂，如此循环。因此同 level-wise 相比，在分裂次数相同的情况下，leaf-wise 可以降低更多的误差，得到更好的精度。leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p>
<p>另一个比较巧妙的优化是 histogram 做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的 k 个桶。利用这个方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>
<hr>
<p>非常赞的工作，实现了和XGBoost不一样的搜索策略，所以在算法效果上并不是完全一样。</p>
<p>- XGBoost在单机默认是exact greedy，搜索所有的可能分割点。分布式是dynamic histogram，每一轮迭代重新estimate 潜在split candidate。<br>- LightGBM和最近的FastBDT都采取了提前histogram binning再在bin好的数据上面进行搜索。在限定好candidate splits,<br>- 主要的速度提升似乎来自于两点。 一个是搜索的时候选取delta比较大的叶子扩展。第二个是pre-bin之后的histogram的求和用了一个非常巧妙的减法trick，省了一半的时间。</p>
<p>在算法和效果上面</p>
<p>最近比较多的工作都开始基于提前限定分割点的近似算法然后快速求histogram。 这一类算法的潜在问题是限制了分割点只能是一开始的定下来的潜在这些。不知道这一点对于实际应用的影响会有多大。理论上数据越多，树越深的时候，需要的潜在分割点越多，可能需要根据训练来动态更新潜在的分割点。</p>
<p>在算法上面采用delta比较大的扩展方向可以集中搜索提高比较重要的区域。一个潜在的问题是可能会忽略掉一些未来有潜力的节点。</p>
<p>当然这些讨论都和具体的应用场景有关。个人觉得exact greedy和histogram方法的还会共存一段时间。或许可以比较好的在系统上面对这两个一起支持</p>
<p>在系统上面</p>
<p>因为集中做针对叶子的分割，似乎会对于数据集的random access有一定的要求。如果不shuffle data的情况下可能会有cache 的问题。这个数据结构是一个有趣的问题</p>
<p>非常值得学习，机器学习系统优化是非常重要的方向。希望可以看到越来越多这样实际的工作</p>
<h4 id="Factorization-Machine-and-Field-Factorization-Machine"><a href="#Factorization-Machine-and-Field-Factorization-Machine" class="headerlink" title="Factorization Machine and Field Factorization Machine"></a>Factorization Machine and Field Factorization Machine</h4><h5 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h5><p>旨在解决稀疏数据下的特征组合问题，具有线性的计算复杂度；（矩阵分解方式处理参数，不仅能减少参数数量，还能处理由于稀疏性带来的参数不好训练的问题）一般的线性模型压根没有考虑特征间的关联(组合).</p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116022611357.png" alt="FM"></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116022630055.png" alt></p>
<h5 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h5><p>通过引入field的概念，FFM把相同性质的特征归于同一个field。在FFM中，每一维特征 xi，针对其它特征的每一种field fj，都会学习一个隐向量 vi,fj。因此，隐向量不仅与特征相关，也与field相关。如果隐向量的长度为 k，那么FFM的二次参数有 nfk 个，远多于FM模型的 nk个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 $O(kn^2)$。</p>
<p> <img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/20170824140534619" alt="FFM"> </p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116023256527.png" alt></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116023321114.png" alt></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191116023414278.png" alt="FFM using SGD"></p>
<p>FFM相对于FM加入了field的概念，每个特征的隐向量不再是只有一个，而是针对每个field学习一个独立的隐向量，防止互相影响。因此理论上来说，在理解业务的基础上，设计合理field之后的FFM模型应该是不会比FM差的。关键在于计算复杂度，FM通过合适的推导，训练/预测复杂度是线性的。而FFM的复杂度是二次方的。在工业界的业务中，如果FFM相对于FM提升不是很明显，一般还是选用FM模型，毕竟计算复杂度差的太多了。</p>
<h4 id="K-Means-and-EM"><a href="#K-Means-and-EM" class="headerlink" title="K-Means and EM"></a>K-Means and EM</h4><p>Choosing $k$ data points as cluster centers, then combining each data into those $k$ clusters by closest distance as metric. Finally, iteratively updating cluster centers by mean of all data of each cluster until no change of those centers.</p>
<p><a href="[https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm](https://en.wikipedia.org/wiki/Expectation–maximization_algorithm)">EM</a> is a powerful algorithm. </p>
<p>Details can be seen in these <a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf" target="_blank" rel="noopener">post</a>, <a href="http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf" target="_blank" rel="noopener">post</a> and <a href="http://bjlkeng.github.io/posts/the-expectation-maximization-algorithm/" target="_blank" rel="noopener">post</a>.</p>
<h4 id="PCA-and-LDA"><a href="#PCA-and-LDA" class="headerlink" title="PCA and LDA"></a>PCA and LDA</h4><p><a href="http://www.svcl.ucsd.edu/courses/ece271B-F09/handouts/Dimensionality2.pdf" target="_blank" rel="noopener">http://www.svcl.ucsd.edu/courses/ece271B-F09/handouts/Dimensionality2.pdf</a></p>
<h4 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h4><h4 id="Spectral-Clustering"><a href="#Spectral-Clustering" class="headerlink" title="Spectral Clustering"></a>Spectral Clustering</h4><p><a href="https://github.com/zhangleiszu/machineLearning/blob/master/A%20Tutorial%20on%20Spectral%20Clustering.pdf" target="_blank" rel="noopener">https://github.com/zhangleiszu/machineLearning/blob/master/A%20Tutorial%20on%20Spectral%20Clustering.pdf</a></p>
<h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p><a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">http://cs231n.stanford.edu/</a></p>
<h4 id="Characteristic"><a href="#Characteristic" class="headerlink" title="Characteristic"></a>Characteristic</h4><ul>
<li>Local feature </li>
<li>Sparse connectivity</li>
<li>Parameter sharing</li>
<li>Shift invariant </li>
</ul>
<h4 id="Different-Structures"><a href="#Different-Structures" class="headerlink" title="Different Structures"></a>Different Structures</h4><ul>
<li>Standard Convolution</li>
<li>Transposed Convolution (Deconvolution)</li>
<li>Dilated Convolution (Atrous Convolution)</li>
<li>Separable Convolution</li>
<li>Gated Convolution</li>
</ul>
<h4 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h4><h5 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h5><p>Advantages:</p>
<ul>
<li>increasing respective fields</li>
<li>shift invariant</li>
<li>reducing number of parameters and difficulty of optimization</li>
</ul>
<p>Pooling provides a sort of shift invariant, however it destroys information of position and space, etc. <a href="https://arxiv.org/pdf/1801.01450.pdf" target="_blank" rel="noopener">Quantifying Translation-Invariance in Convolutional Neural Networks</a> shows that CNN itself has no shift invariant which is learnt through data. </p>
<p>Another research from DeepMind, <a href="https://arxiv.org/pdf/1804.04438.pdf" target="_blank" rel="noopener">Pooling is neither necessary nor sufficient for appropriate deformation stability in CNNs</a>, shows that models can still get great performance without pooling. What’s more, they think that smoother of  parameters of CNN kernels can make shift more stable.</p>
<h5 id="Stochastic-Deep"><a href="#Stochastic-Deep" class="headerlink" title="Stochastic Deep"></a>Stochastic Deep</h5><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><h4 id="Basic-Structure"><a href="#Basic-Structure" class="headerlink" title="Basic Structure"></a>Basic Structure</h4><p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191115220219342.png" alt="RNN"><br>$$<br>\Rightarrow a^{(t)} = W \cdot [h^{(t-1)};x^{(t)}]+b \<br>\Rightarrow h^{(t)} = f(a^{(t)}) \<br>\Rightarrow h^{(t)}=f(h^{(t-1)},x^{(t)};\theta) \<br>$$<br>$Tanh$ is always the activation function.</p>
<p><a href="https://www.zhihu.com/question/61265076" target="_blank" rel="noopener">Why do we use tanh as activation function but ReLU?</a></p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191115220304740.png" alt="LSTM"></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191118002139210.png" alt="forget gate"></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191118002146556.png" alt="input gate"></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191118002155566.png" alt="cell status"></p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191118002203027.png" alt="output gate and hidden status"></p>
<h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p><a href="https://medium.com/mlrecipies/deep-learning-basics-gated-recurrent-unit-gru-1d8e9fae7280" target="_blank" rel="noopener">https://medium.com/mlrecipies/deep-learning-basics-gated-recurrent-unit-gru-1d8e9fae7280</a></p>
<p>It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. </p>
<p><img src="/2019/12/11/Study-Notes-of-Machine-Learning-V1/image-20191115220329260.png" alt="GRU"></p>
<h4 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h4><p><a href="https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45" target="_blank" rel="noopener">https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>On the Number of Linear Regions of Deep Neural Networks</title>
    <url>/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h3><p>使用ReLU等非线性单元的神经网络相当于一个分片线性函数，线性区域越多，神经网络的非线性就越强，网络的效果可能就更好。这篇文章讨论了在神经元总数一样的情况下，增加网络的深度可以产生更多的线性区域。文章中使用折纸类比了这个过程。</p><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209181524633.png" title="李宏毅Deep Structure PPT"><p>以ReLU为例，我们知道ReLU单元$y=ReLU(Wx+b)$会在输入空间中产生一个超平面$Wx+b=0$,把输入空间分成两部分，一部分的空间输出值为常数0，另一个空间的输出为$y=Wx+b$。可以看到一个分片线性函数会产生两个线性区域。那么如果输入空间是二维，就像一张纸沿着$Wx+b=0$折一下，再打开，会产生两个线性区域。</p><p>对于一个具有两个ReLU单元的单隐藏层神经网络，$y_1=ReLU(W_1x+b_1)$和$y_2=ReLU(W_2x+b_2)$，输出为$y=y_1+y_2$。那么这个神经网络就有四个线性区域，每个区域的输出值为0、$W_1x+b_1$、$W_2x+b_2$和$(W_1+W_2)x+(b_1+b_2)$。这就像一张纸，先沿着$W_1x+b_1=0$折一下，打开，再沿着$W_2x+b_2=0$折一下，会产生四个区域。所以如果只有一个隐藏层，单纯地增加神经元的个数，相当于重复上述操作，不断的沿着一条线折一下，再打开。对于一张二维的纸而言，折叠N此所产生的线性区域不超过$\frac{N^2+N}{2}+1$.</p><a id="more"></a>
                        



<p>对于两个隐藏层的神经网络。第二层的一个ReLU单元会在第二层的输入空间（第一层的输出值域）上形成一个超平面，并将第一层的值域分成两个区域。而我们注意到，第一层可能会把不同的输入映射到相同的输出值，因此在第一层的值域上又多一个区域，就可能再输入空间上多出好几个区域。这就像拿一张纸，折几下，折出几个区域，然后不打开，再折一下。那么最后折的这一下，就会在之前折出的多个区域中新增一个区域。这就是增加层数带来的效果。对于更深层的网络，就像拿一张纸，一直折N次，折到最后才打开，那么这些折痕最多可以把纸分割成$2^N$个区域。</p>
<h3 id="1-Introduction-amp-Conclusion"><a href="#1-Introduction-amp-Conclusion" class="headerlink" title="1. Introduction &amp; Conclusion"></a>1. Introduction &amp; Conclusion</h3><p>这篇文章研究了深度前馈神经网络可根据它们的线性区域数计算函数的复杂性；讨论了深层模型的每一层都能够识别其输入片段，使得层与层的组合能够识别指数数量的输入区域。这导致指数地复制在更高层中计算的函数的复杂性。深度模型以这种方式计算的函数是复杂的，但是它们仍然具有由复制引起的intrinsic rigidity，这可能有助于深度模型比浅模型更好地泛化到看不见的样本。线性区域的结构取决于单元的类型，例如hyperplane arrangements for shallow rectifier和Voronoi diagrams for shallow maxout networks。在maxout和rectifier网络中，随着深度增加区域数量都指数地增长。给定网络的参数空间被划分为目标函数具有相应线性区域的区域。</p>
<p>在许多隐藏层的渐近极限中，尽管使用相同数量的计算单元，深层网络能够将它们的输入空间分成比浅层网络指数地更线性的响应区域。这篇文章描述了那些模型的中间层如何将它们的几个输入映射到同一个输出。以这种方式计算的函数的分层组合经常随着层数的增加而指数地重复使用低级计算。这一关键属性使深层网络能够计算高度复杂和结构化的函数。通过估计可由两种重要类型的分段线性网络（rectifier units和maxout units）计算的函数的线性区域的数量来支持这个想法。</p>
<p>给定模型可以计算的函数的线性区域的数量是模型灵活性的度量。如图1所示，比较了相同隐藏神经元数量的单层和双层网络学到的决策边界，体现了深度的优势；深层模型更精确地捕捉到所需的边界，用大量的线性pieces来逼近它。深层网络能够通过将输入邻域映射到中间隐藏层的一个公共输出来识别指数数量的输入邻域。对该中间层的激活的计算被复制多次，在每个被识别的邻域中复制一次。这允许网络计算非常复杂的函数，即使它们是用相对较少的参数定义的。参数的数量是可由网络计算的函数的维度的上限，而少量参数意味着可计算函数具有低维度。深度前馈分段线性网络可计算的一组函数即使维度很低，但通过在层与层之间重复使用和组合特征，实现了指数复杂性。</p>
<img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191208183425173.png">

<h3 id="2-Feedforward-Neural-Networks-and-their-Compositional-Properties"><a href="#2-Feedforward-Neural-Networks-and-their-Compositional-Properties" class="headerlink" title="2.Feedforward Neural Networks and their Compositional Properties"></a>2.Feedforward Neural Networks and their Compositional Properties</h3><p>本节讨论了深度前馈网络通过使用相对较少的计算单元来重新映射其输入空间以创建复杂对称性的能力。分析了深度模型的每一层都能够将其输入的不同区域映射到一个公共输出。这导致了一种复合结构，其中在给定层上产生相同输出的所有输入区域中，有效地复制了较高层上的计算。在输入空间上复制计算的能力随网络层的数量呈指数增长。</p>
<h4 id="2-1-Shallow-Neural-Networks"><a href="#2-1-Shallow-Neural-Networks" class="headerlink" title="2.1 Shallow Neural Networks"></a>2.1 Shallow Neural Networks</h4>

根据输入的不同，Rectifier units有两种行为，等于0或者线性。这两个行为之间的边界由超平面给出，来自rectifier层中所有单元的所有超平面的集合形成一个hyperplane arrangement。arrangement中的超平面将输入空间分为几个区域。arrangement的区域数量可以根据arrangement的特征函数给出。在$R^{n_0}$中一个$n_1$的arrangement有最多$\sum_{j=0}^{n_0} {{n_1} \choose j}$个区域。当且仅当超平面处于一般位置时，才能实现这个数量的区域。这意味着由shallow rectifier network（$n_0$的输入和$n_1$的隐藏单元）计算的函数的线性区域的最大数量是$\sum_{j=0}^{n_0} {n_1 \choose j}$。



<h4 id="2-2-Deep-Neural-Networks"><a href="#2-2-Deep-Neural-Networks" class="headerlink" title="2.2 Deep Neural Networks"></a>2.2 Deep Neural Networks</h4><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209001503997.png">

<p>由前馈网络的L层对L-1层的一组激活进行的计算可有效地对导致L-1层相同激活的所有输入空间区域进行。可以选择给定层的输入权重和偏差，使得计算的函数在输入空间中具有最大数量的preimages的前一层的那些激活值上表现出最有趣的行为，从而在输入空间中多次重复有趣的计算和生成complicated-looking的函数。</p>
<img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209003254040.png">

<img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209003328371.png">

<p>构造具有许多线性区域的函数的想法是使用前L-1个隐藏层来标识许多输入空间邻域，并将它们全部映射到L-1隐层的激活邻域$P^L$，每个激活邻域$P^L$都属于一个不同的最后一个隐藏层的线性区域。</p>
<h4 id="2-34-Identification-of-Inputs-as-Space-Foldings"><a href="#2-34-Identification-of-Inputs-as-Space-Foldings" class="headerlink" title="2.34 Identification of Inputs as Space Foldings"></a>2.34 Identification of Inputs as Space Foldings</h4><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209003729020.png">

<p>识别两个子集$S$和$S’$的映射$F$可以看成是折叠它domain的一种算子，折叠后$S$和$S’$被映射到相同的输出。如图2所示，absolute value function折叠它自己的domain两次（两个坐标轴各一次）。这种折叠识别了二维欧几里德空间的四个象限。通过组成这样的操作，相同类型的映射可以再次应用于输出，以便重新折叠之前的第一次折叠。深层神经网络的每个隐藏层都和一种折叠算子相关。每个隐藏层折叠前一层的激活空间。反过来，深层神经网络从第一层开始有效地递归折叠其输入空间。这种递归折叠的结果是，在最终折叠空间上计算的任何函数都将应用于由对应于连续折叠的映射所识别的所有的折叠子集。这意味着在深度模型中，最后一层的图像空间的任何划分都被复制到由一连串折叠识别的所有输入空间区域中。</p>
<p>空间折叠不限于沿坐标轴折叠，也不必保留长度。相反，空间被折叠是根据输入权重$W$和偏置$b$中编码的方向和偏移以及在每个隐藏层使用的非线性激活函数。这意味着所识别的输入空间区域的大小和方向可以彼此不同。在激活函数不是分段线性的情况下，折叠操作可能更加复杂。</p>
<img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209130708933.png">

<h3 id="3-Deep-Rectifier-Network"><a href="#3-Deep-Rectifier-Network" class="headerlink" title="3.Deep Rectifier Network"></a>3.Deep Rectifier Network</h3><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209133906134.png">

<h4 id="3-1-Illustration-of-the-Construction"><a href="#3-1-Illustration-of-the-Construction" class="headerlink" title="3.1 Illustration of the Construction"></a>3.1 Illustration of the Construction</h4><p>考虑一层网络有$n$个rectifiers，$n_0$个输入变量，其中$n \geq n_0$。将rectifier单元集划分为基数为$p=[n / n_0]$的$n_0$个(非重叠)子集，忽略多余单元。考虑第$j$个子集中的单元。</p>
<img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209140301680.png">

<p>$\tilde h$是由$h$通过与线性函数组合产生的。这个线性函数可以有效地被下一层的预激活函数吸收。因此，我们可以将$\tilde h$视为由当前层计算的函数。作为此rectifier层的单位超立方体输出的函数，由较深层进行的计算将复制到$p^{n_0}$个已识别输入空间超立方体的每一个上。</p>
<h4 id="3-2-Formal-Result"><a href="#3-2-Formal-Result" class="headerlink" title="3.2 Formal Result"></a>3.2 Formal Result</h4><p>可以将上面所描述的construction泛化为一个深层rectifier网络，有$n_0$的输入和$L$层宽度为$n_i$的隐藏层，对于所有$i \in [L]$，$n_i \geq n_0$。那么这个深层rectifier网络的线性区域的最大数量的下限为：</p>
<img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209153956151.png">

<p>下一个推论给出这些边界的渐近行为的表达式。假设对于所有$i \geq 1$都有$n_0=\Omega (1)$和$n_i=n_0$，一个有$Ln$个隐藏单元的单层网络的区域数量表现为$\Omega (L^{n_0}n^{n_0})$。但是，对于一个深度网络，</p>
<img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209154657346.png">

<p>因此，深层模型的线性区域数量呈指数增长(in $L$)，以及呈多项式增长(in $n$)，这比具有$nL$个隐藏单元的浅层模型的线性区域快得多。结果表明，即使$L$和$n$较小，深的rectifier模型也比浅的rectifier模型能够产生更多的线性区域。</p>
<h3 id="4-Deep-Maxout-Networks"><a href="#4-Deep-Maxout-Networks" class="headerlink" title="4.Deep Maxout Networks"></a>4.Deep Maxout Networks</h3><img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209155554775.png">

<p>由于两个凸函数的最大值为凸，因此maxout单元和maxout层将计算凸函数。函数集合的最大值被称为<em>upper envelope</em>。一个maxout单元线性区域的最大数量等于它的rank(上式中的k)。</p>
<p>maxout层的线性区域是各个maxout单元的线性区域的交集。为了获得该层的线性区域的数量，需要描述每个maxout单元的线性区域的结构，并研究它们可能的交集。Voronoi图可以提升到线性函数的upper envelopes，因此它们描述了由maxout单元生成的输入空间分区。</p>
<img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209161748533.png">

<p>可以通过一个具有两倍数量单元的rectifier层来模拟一个rank-2 maxout层。一个有$L-1$个宽度$n=n_0$的隐藏层的rank-2 maxout网络可以识别出$2^{n_0(L-1)}$个输入空间区域。反过来看，它可以计算有$2^{n_0(L-1)}2^{n_0}=2^{n_0L}$个线性区域的函数。对于rank-k，一个rank-k maxout单元可以从它的输入域识别k个cones，每个cone都与一个对应线性方程$f_i$的梯度$W_i$的positive half-ray ${rW_i \in R^n: r \in R_+ }$相邻。</p>
<img src="/2019/12/09/On-the-Number-of-Linear-Regions-of-Deep-Neural-Networks/image-20191209162932406.png">

<p>所以深层maxout网络可以计算具有多个线性区域的函数，这些线性区域随层数成指数增长，并且比具有相同数量单元的浅层模型的最大区域数更快地成指数增长。与rectifier模型相似，该指数行为也可以根据网络参数的数量来确定。尽管可以由maxout层计算的某些函数也可以由rectifier层计算，但是从最后一节开始的rectifier构造会出现maxout网络无法计算的函数。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://www.zhihu.com/question/62705160/answer/201377484" target="_blank" rel="noopener">https://www.zhihu.com/question/62705160/answer/201377484</a></p>
<p>[2] <a href="https://arxiv.org/pdf/1402.1869.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1402.1869.pdf</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么神经网络越深越好</title>
    <url>/2019/11/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近在StackExchange上看到的一个帖子，<a href="https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w" target="_blank" rel="noopener">What is the difference between a neural network and a deep neural network, and why do the deep ones work better?</a>，讨论的是“深度”对于神经网络的影响。从下面的图片可以看到ImageNet中的冠军模型也越来越深。</p><img src="/2019/11/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD/20191124-1.png" width="80%" height="80%" title="ImageNet" alt="ImageNet"><p>在Deep Learning那本书中，我们也能看到一样的结果。</p><img src="/2019/11/24/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%B7%B1%E8%B6%8A%E5%A5%BD/20191124-2.png" width="80%" height="80%" title="Book: Deep Learning" alt="Book: Deep Learning"><p>为什么网络越深模型的表现越好呢？下面是不同角度对这个问题的回答。</p><p>在知乎的帖子<a href="https://www.zhihu.com/question/313633835" target="_blank" rel="noopener">网络深度对深度学习模型性能有什么影响？</a>中，答主言有三从两个方面解释了<u>为什么加深可以提升性能</u>：<strong>1.更好的拟合特征，更强大的表达能力。更深的模型，意味着更好的非线性表达能力，可以学习更加复杂的变换，从而可以拟合更加复杂的特征输入。2.网络更深，每一层要做的事情也更加简单了，可以更好地进行逐层学习。</strong></p><a id="more"></a>







<p>同时他也提到了<u>模型变深后带来的两个问题</u>：<strong>1.加深带来的优化问题。深层网络带来的梯度不稳定，网络退化的问题始终都是存在的，可以缓解，没法消除。这就有可能出现网络加深，性能反而开始下降。2.网络加深带来的饱和。随着深度的增加，模型的提升效果逐渐饱和。除此之外，模型加深还可能出现的一些问题是导致某些浅层的学习能力下降，限制了深层网络的学习，这也是跳层连接等结构能够发挥作用的很重要的因素。</strong>模型变深怎么也绕不开ResNet，最近看了很多关于ResNet的文章，对它有了一些新的认识，后面会写一篇ResNet的文章分享。</p>
<p>另一篇知乎的帖子<a href="https://www.zhihu.com/question/62705160/answer/201377484" target="_blank" rel="noopener">如何形象的解释深度学习神经网络需要更深而非更广？</a>，一个答主引用Yoshua Begio的paper-<a href="https://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf" target="_blank" rel="noopener">On the Number of Linear Regions of Deep Neural Networks</a>的观点（前面的那篇帖子中也提到了这个，后面会写一篇这篇paper的总结）。主要的观点是：<strong>使用ReLU、MaxOut等非线性单元的神经网络在数学上相当于一个分片线性函数，线性区域(linear region)越多，神经网络的非线性就越强，也就更有可能在实际任务中取得好的效果。</strong></p>
<p>另一个角度的解释是</p>
<blockquote>
<p>通常来说，卷积核的大小通常为3*3或者5*5，相对于图片几百乘几百的大小来说，那么小的卷积核一次只能覆盖很小的区域，所以浅层小卷积核只能探测到诸如边或者角这种比较低层次的特征。<strong>随着网络深度的增加，卷积核的有效感受野越来越大，也就意味着高层的卷积核能够覆盖更大尺度上的特征</strong>。也可以认为高层的卷积核将低层卷积核学习到的特征进行了组合。那么问题来了，增加宽度我们说是增加了某一层卷积核的个数，也就意味着探测到的低层次特征理论上变多了。如果仅仅是增加宽度而深度不够，那么最后输出的高层特征仅仅组合了较少的低层特征，比如低层的卷积核探测到了眼睛鼻子和尾巴，第四层可以把眼睛和鼻子组合起来，但是尾巴距离眼睛比较远，可能需要第六层的卷积核的有效感受野才能同时探测到尾巴和眼睛，所以说如果现在网络只有四层，很有可能不能结合距离较远的两个特征。所以说深度很重要，因为更深的卷积核的有效感受野越大，能够组合的低层特征越多。图像分类任务最终的分类依据应该是丰富的低层特征组合而不是分散的低层特征。</p>
<p>作者：Terence Wu</p>
</blockquote>
<p>综上所述，模型越来越深背后的理论和实践解释：</p>
<ol>
<li>“a naive answer is that because they work better”；</li>
<li>更深的模型可以更好的拟合特征；</li>
<li>网络更深，每一层要做的事情也更加简单了，可以更好地进行逐层学习；</li>
<li>使用非线性单元（RELU、Maxout等）的神经网络，网络越深，特征空间被分割得到的线性区域越多，模型的表现越好。</li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
</search>
